{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#what-is-kube-burner","title":"What is Kube-burner","text":"<p>Kube-burner is a Kubernetes performance and scale test orchestration toolset. It provides multi-faceted functionality, the most important of which are summarized below.</p> <ul> <li>Create, delete, read, and patch Kubernetes resources at scale.</li> <li>Prometheus metric collection and indexing.</li> <li>Measurements.</li> <li>Alerting.</li> </ul> <p>Kube-burner is a binary application written in Golang that makes extensive usage of the official k8s client library, client-go.</p> <p></p>"},{"location":"#quick-starting-with-kube-burner","title":"Quick starting with kube-burner","text":"<p>To start tinkering with kube-burner now:</p> <ul> <li>Find binaries for different CPU architectures and operating systems in the releases section of the repository.</li> <li>Use the container image repository available at quay.</li> <li>Reference valid examples of configuration files, metrics profiles, and Grafana dashboards in the examples directory of the repository.</li> </ul>"},{"location":"ocp/","title":"OpenShift Plugin","text":"<p>While Kube-burner supports operations operations on vanilla k8s, it is also capable of being run against a wide variety of distributions. It is possible to leverage its core functionality to create custom distribution focused plugins that come with canned scenarios tailored to that distribution as well any unique logic or operations. An example of this is the OpenShift Plugin shipped as the kube-burner-ocp binary with OpenShift specific scenarios, templates and objects in a ready to use format. Please refer to the repository for further information on the plugin and how it can be run against OpenShift clusters.</p>"},{"location":"cli/","title":"CLI","text":"<p>kube-burner is a tool written in Golang that can be used to stress Kubernetes clusters by creating, deleting, and patching resources at a given rate. The actions taken by this tool are highly customizable and their available subcommands are detailed below:</p> <pre><code>$ kube-burner help\nKube-burner \ud83d\udd25\n\nTool aimed at stressing a kubernetes cluster by creating or deleting lots of objects.\n\nUsage:\n  kube-burner [command]\n\nAvailable Commands:\n  check-alerts Evaluate alerts for the given time range\n  completion   Generates completion scripts for bash shell\n  destroy      Destroy old namespaces labeled with the given UUID.\n  health-check Check for Health Status of the cluster\n  help         Help about any command\n  import       Import metrics tarball\n  index        Index kube-burner metrics\n  init         Launch benchmark\n  measure      Take measurements for a given set of resources without running workload\n  version      Print the version number of kube-burner\n\nFlags:\n  -h, --help               help for kube-burner\n      --log-level string   Allowed values: debug, info, warn, error, fatal (default \"info\")\n\nUse \"kube-burner [command] --help\" for more information about a command.\n</code></pre>"},{"location":"cli/#init","title":"Init","text":"<p>This is the main subcommand; it triggers a new kube-burner benchmark and it supports the these flags:</p> <ul> <li><code>uuid</code>: Benchmark ID. This is essentially an arbitrary string that is used for different purposes along the benchmark. For example, to label the objects created by kube-burner as mentioned in the reference chapter. By default, it is auto-generated.</li> <li><code>config</code>: Path or URL to a valid configuration file. See details about the configuration schema in the reference chapter.</li> <li><code>configmap</code>: In case of not providing the <code>--config</code> flag, kube-burner is able to fetch its configuration from a given <code>configMap</code>. This variable configures its name. kube-burner expects the configMap to hold all the required configuration: config.yml, metrics.yml, and alerts.yml. Where metrics.yml and alerts.yml are optional.</li> <li><code>namespace</code>: Name of the namespace where the configmap is.</li> <li><code>log-level</code>: Logging level, one of: <code>debug</code>, <code>error</code>, <code>info</code> or <code>fatal</code>. Default <code>info</code>.</li> <li><code>metrics-endpoint</code>: Path to a valid metrics endpoint file.</li> <li><code>skip-tls-verify</code>: Skip TLS verification for Prometheus. The default is <code>true</code>.</li> <li><code>timeout</code>: Kube-burner benchmark global timeout. When timing out, return code is 2. The default is <code>4h</code>.</li> <li><code>kubeconfig</code>: Path to the kubeconfig file.</li> <li><code>kube-context</code>: The name of the kubeconfig context to use.</li> <li><code>user-metadata</code>: YAML file path containing custom user-metadata to be indexed along with the <code>jobSummary</code> document.</li> <li><code>user-data</code>: YAML or JSON file path containing input variables for rendering the configuration file.</li> <li><code>allow-missing</code>: Allow missing keys in the config file. Needed when using the <code>default</code> template function</li> </ul> <p>Prometheus authentication</p> <p>Both basic and token authentication methods need permissions able to query the given Prometheus endpoint.</p> <p>With the above, running a kube-burner benchmark would be as simple as:</p> <pre><code>kube-burner init -c cfg.yml --uuid 67f9ec6d-6a9e-46b6-a3bb-065cde988790`\n</code></pre> <p>Kube-burner also supports remote configuration files served by a web server. To use it, rather than a path, pass a URL. For example:</p> <pre><code>kube-burner init -c http://web.domain.com:8080/cfg.yml --uuid 67f9ec6d-6a9e-46b6-a3bb-065cde988790`\n</code></pre> <p>To scrape metrics from multiple endpoints, the  <code>init</code> command can be triggered. For example:</p> <pre><code>kube-burner init -c cluster-density.yml -e metrics-endpoints.yaml\n</code></pre> <p>A metrics-endpoints.yaml file with valid keys for the <code>init</code> command would look like the following:</p> <pre><code>- endpoint: http://localhost:9090\n  token: &lt;token&gt;\n  metrics: [metrics.yaml]\n  indexer:\n    type: local\n- endpoint: http://remotehost:9090\n  username: foo\n  password: bar\n  alerts: [alert-profile.yaml]\n</code></pre>"},{"location":"cli/#exit-codes","title":"Exit codes","text":"<p>Kube-burner has defined a series of exit codes that can help to programmatically identify a benchmark execution error.</p> Exit code Meaning 0 Benchmark execution finished normally 1 Generic exit code, returned on a unrecoverable error (i.e: API Authorization error or config parsing error) 2 Benchmark timeout, returned when kube-burner's execution time exceeds the value passed in the <code>--timeout</code> flag 3 Alerting error, returned when a <code>error</code> or <code>critical</code> level alert is fired 4 Measurement error, returned on some measurements error conditions, like <code>thresholds</code>"},{"location":"cli/#index","title":"Index","text":"<p>This subcommand can be used to collect and index the metrics from a given time range. The time range is given by:</p> <ul> <li><code>start</code>: Epoch start time. Defaults to one hour before the current time.</li> <li><code>end</code>: Epoch end time. Defaults to the current time.</li> </ul>"},{"location":"cli/#measure","title":"Measure","text":"<p>This subcommand can be used to collect measurements for a given set of resources which were part of a workload ran in past and are still present on the cluster (i.e only supports podLatency as of today). We can specify a list of namespaces and selector labels as input.</p> <ul> <li><code>namespaces</code>: comma-separated list of namespaces provided as a string input. This is optional, by default all namespaces are considered.</li> <li><code>selector</code>: comma-separated list of selector labels in the format key1=value1,key2=value2. This is optional, by default no labels will be used for filtering.</li> </ul> <p>Note</p> <p>This subcommand should only be used to fetch measurements of a workload ran in the past. Also those resources should be active on the cluster. For present cases, please refer to the alternate options in this tool.</p>"},{"location":"cli/#check-alerts","title":"Check alerts","text":"<p>This subcommand can be used to evaluate alerts configured in the given alert profile. Similar to <code>index</code>, the time range is given by the <code>start</code> and <code>end</code> flags.</p>"},{"location":"cli/#destroy","title":"Destroy","text":"<p>This subcommand requires the <code>uuid</code> flag to destroy all namespaces labeled with <code>kube-burner-uuid=&lt;UUID&gt;</code>.</p>"},{"location":"cli/#health-check","title":"Health Check","text":"<p>The <code>health-check</code> subcommand assesses the status of nodes within the cluster. It provides information on the overall health of the cluster, indicating whether it is in a healthy state. In the event of an unhealthy cluster, the subcommand returns a list of nodes that are not in a \"Ready\" state, helping users identify and address specific issues affecting cluster stability.</p>"},{"location":"cli/#completion","title":"Completion","text":"<p>Generates bash a completion script that can be imported with: <code>. &lt;(kube-burner completion)</code></p> <p>Or permanently imported with: <code>kube-burner completion &gt; /etc/bash_completion.d/kube-burner</code></p> <p>Note</p> <p>the <code>bash-completion</code> utils must be installed for the kube-burner completion script to work.</p>"},{"location":"contributing/","title":"Contributing to kube-burner","text":"<p>If you want to contribute to kube-burner, you can do so by submitting a Pull Request, Issue or starting a Discussion. You can also reach us out in the <code>#kube-burner</code> channel of the kubernetes slack.</p>"},{"location":"contributing/#ci-and-linting","title":"CI and Linting","text":"<p>For running pre-commit checks on your code before committing code and opening a PR, you can use the <code>pre-commit run</code> functionality.  See CI docs for more information on running pre-commits.</p>"},{"location":"contributing/#building","title":"Building","text":"<p>To build kube-burner just execute <code>make build</code>, once finished the kube-burner binary should be available at <code>./bin/&lt;arch&gt;/kube-burner</code>.</p> <p>Note</p> <p>Building kube-burner requires <code>golang &gt;=1.19</code></p> <pre><code>$ make build\nBuilding bin/amd64/kube-burner\nGOPATH=/home/rsevilla/go/\nGOARCH=amd64 CGO_ENABLED=0 go build -v -ldflags \"-X github.com/cloud-bulldozer/go-commons/version.GitCommit=4c9c3f43db83adb053efc58220ddd696d1d19a35 -X github.com/cloud-bulldozer/go-commons/version.BuildDate=2024-01-10-21:24:20 -X github.com/cloud-bulldozer/go-commons/version.Version=main\" -o bin/amd64/kube-burner ./cmd/kube-burner\ngithub.com/kube-burner/kube-burner/cmd/kube-burner\n</code></pre>"},{"location":"contributing/pullrequest/","title":"Pullrequest","text":"<p>The pull Request Workflow, defined in the <code>ci-tests.yml</code> file, is triggered on <code>pull_request_target</code> events to the branches <code>master</code> and <code>main</code> has three jobs: linters, build and  tests.</p> <pre><code>graph LR\n  A[pull_request_target] --&gt; B[linters];\n  B --&gt; C[build];\n  C --&gt; D[tests];</code></pre>"},{"location":"contributing/pullrequest/#linters","title":"Linters","text":"<p>This job performs the following steps:</p> <ol> <li>Checks out the code</li> <li>Installs pre-commit.</li> <li>Runs pre-commit hooks to execute code linting based on <code>.pre-commit-config.yaml</code> file</li> </ol> <p>Linters can be executed locally with just <code>make lint</code></p> <p>Info</p> <p>Main purpose for pre-commit is to allow developers to pass the Lint Checks before commiting the code. Same checks will be executed on all the commits once they are pushed to GitHub</p> <p>Requirements:</p> <ul> <li>make</li> <li>pre-commit</li> </ul> <pre><code>$ make lint\nExecuting pre-commit for all files\npre-commit run --all-files\ngolangci-lint............................................................Passed\nmarkdownlint.............................................................Passed\nTest shell scripts with shellcheck.......................................Passed\ncheck json...............................................................Passed\npre-commit executed.\n</code></pre>"},{"location":"contributing/pullrequest/#build","title":"Build","text":"<p>The \"build\" job uses the file <code>builders.yml</code> file to build binaries and images, performing the following steps:</p> <ol> <li>Sets up Go 1.19.</li> <li>Checks out the code.</li> <li>Builds the code</li> <li>Builds container images</li> <li>Builds the documentation</li> <li>Installs the built artifacts</li> <li>Uploads the built binary file as an artifact named kube-burner.</li> </ol>"},{"location":"contributing/pullrequest/#tests","title":"Tests","text":"<p>The testing Workflow, defined in the <code>tests-k8s.yml</code> file, runs tests defined in the <code>test</code> directory of the repository.</p> <p>Tests are orchestrated with bats</p> <p>Tests can be executed locally with <code>make test</code>, some requirements are needed though:</p> <ul> <li>make</li> <li>bats</li> <li>kubectl</li> <li>podman or docker (required to run kind)</li> </ul>"},{"location":"contributing/pullrequest/#running-test-with-podman","title":"Running test with Podman","text":"<p>Since the test suite includes KubeVirt VMs, it must run with rootful Podman. Either run the tests as root, or access the rootful Podman socket.</p>"},{"location":"contributing/pullrequest/#allow-access-to-the-rootful-podman-socket","title":"Allow access to the rootful Podman socket","text":"<p>Assuming the user is in <code>wheel</code> group please do the following (one time):</p> <p>As root, create a Drop-In file <code>/etc/systemd/system/podman.socket.d/10-socketgroup.conf</code> with the following content: <pre><code>[Socket]\nSocketGroup=wheel\nExecStartPost=/usr/bin/chmod 755 /run/podman\n</code></pre></p> <p>The 1<sup>st</sup> line is needed in order to create the socket accessible by the <code>wheel</code> group. 2<sup>nd</sup> line because systemd-tmpfiles recreates the folder as root:root without group reading rights.</p> <p>Stop <code>podman.socket</code> if it is running, reload the daemon <code>systemctl daemon-reload</code> since we changed the systemd settings and restart it again <code>systemctl enable --now podman.socket</code></p>"},{"location":"contributing/pullrequest/#running-the-test","title":"Running the test","text":"<p>Instruct Podman to communicate with the rootful Podman socket by setting the environment variable: <pre><code>CONTAINER_HOST=unix://run/podman/podman.sock make test-k8s\n</code></pre></p>"},{"location":"contributing/pullrequest/#running-test-with-an-external-cluster","title":"Running test with an External Cluster","text":"<p>By default, the tests start a local cluster using kind. Instead, you can use an already existsing cluster.</p>"},{"location":"contributing/pullrequest/#prerequisites","title":"Prerequisites","text":"<p>Since the test suite includes KubeVirt VMs, the cluster must include the <code>kubevirt</code> operator. See the Installation guide for details.</p>"},{"location":"contributing/pullrequest/#kubeconfig","title":"Kubeconfig","text":"<p>Either save the kubeconfig file under <code>~/.kube/config</code> or set the environment variable <code>KUBECONFIG</code> to its location</p>"},{"location":"contributing/pullrequest/#run-the-tests","title":"Run the tests","text":"<p>In order to instruct the tests to use the existing cluster set <code>USE_EXISTING_CLUSTER=yes</code> when calling <code>make</code>.</p>"},{"location":"contributing/pullrequest/#execute-a-subset-of-the-tests","title":"Execute a subset of the tests","text":"<p>The list of executed tests may be filtered by setting the environment variable <code>TEST_FILTER</code>:</p> <pre><code>TEST_FILTER=\"datavolume\" make test-k8s\n</code></pre>"},{"location":"contributing/release/","title":"Release","text":"<p>The Release workflow, defined in the <code>release.yml</code> file, when a new tag is pushed it triggers: the workflows defined <code>ci-tests.yml</code>, <code>gorelease.yml</code>, <code>image-upload.yml</code> and <code>docs.yml</code> are executed following the order below:</p> <pre><code>graph LR\ngraph LR\n  A[new tag pushed] --&gt; B[ci-tests];\n  B --&gt; C[gorelease];\n  B --&gt; E[docs];\n  B --&gt; D[image-upload];</code></pre>"},{"location":"contributing/release/#release-build","title":"Release Build","text":"<p>This job uses the <code>Create a new release of project</code> workflow defined in <code>gorelease.yml</code> to create a new release of the project performing the following steps:</p> <ol> <li>Checks out the code into the Go module directory.</li> <li>Sets up Go 1.19.</li> <li>Runs GoReleaser to create a new release, including the removal of previous distribution files.</li> </ol>"},{"location":"contributing/release/#image-upload","title":"Image Upload","text":"<p>This job uses the <code>Upload Containers to Quay</code> workflow defined in <code>image-upload.yml</code> to upload containers to the Quay registry for multiple architectures (arm64, amd64, ppc64le, s390x) performing the following steps:</p> <ol> <li>Installs the dependencies required for multi-architecture builds.</li> <li>Checks out the code.</li> <li>Sets up Go 1.19.</li> <li>Logs in to Quay using the provided QUAY_USER and QUAY_TOKEN secrets.</li> <li>Builds the kube-burner binary for the specified architecture.</li> <li>Builds the container image using the make images command, with environment variables for architecture and organization.</li> <li>Pushes the container image to Quay using the make push command.</li> </ol> <p>The \"manifest\" job builds a container manifest and runs after the \"containers\" job. It performs the following steps:</p> <ol> <li>Checks out the code.</li> <li>Logs in to Quay using the provided QUAY_USER and QUAY_TOKEN secrets.</li> <li>Creates and pushes the container manifest using the make manifest command, with the organization specified.</li> </ol>"},{"location":"contributing/release/#docs-update","title":"Docs Update","text":"<p>Uses the <code>Deploy docs</code> workflow defined in <code>docs.yml</code> to generate and deploy the documentation performing the following steps:</p> <ol> <li>Checks out the code.</li> <li>Sets up Python 3.x.</li> <li>Exports the release tag version as an environment variable.</li> <li>Sets up the Git configuration for documentation deployment.</li> <li>Installs the required dependencies, including mkdocs-material and mike.</li> <li>Deploys the documentation using the mike deploy command, with specific parameters for updating aliases and including the release tag version in the deployment message.</li> </ol>"},{"location":"contributing/tests/","title":"Tests","text":"<p>The testing Workflow, defined in the <code>tests-k8s.yml</code> file, runs tests defined in the <code>test</code> directory of the repository.</p> <p>Tests are orchestrated with bats</p> <p>Tests can be executed locally with <code>make test</code>, some requirements are needed though:</p> <ul> <li>make</li> <li>bats</li> <li>kubectl</li> <li>podman or docker (required to run kind)</li> </ul>"},{"location":"contributing/tests/#running-test-with-podman","title":"Running test with Podman","text":"<p>Since the test suite includes KubeVirt VMs, it must run with rootful Podman. Either run the tests as root, or access the rootful Podman socket.</p>"},{"location":"contributing/tests/#allow-access-to-the-rootful-podman-socket","title":"Allow access to the rootful Podman socket","text":"<p>Assuming the user is in <code>wheel</code> group please do the following (one time):</p> <p>As root, create a Drop-In file <code>/etc/systemd/system/podman.socket.d/10-socketgroup.conf</code> with the following content: <pre><code>[Socket]\nSocketGroup=wheel\nExecStartPost=/usr/bin/chmod 755 /run/podman\n</code></pre></p> <p>The 1<sup>st</sup> line is needed in order to create the socket accessible by the <code>wheel</code> group. 2<sup>nd</sup> line because systemd-tmpfiles recreates the folder as root:root without group reading rights.</p> <p>Stop <code>podman.socket</code> if it is running, reload the daemon <code>systemctl daemon-reload</code> since we changed the systemd settings and restart it again <code>systemctl enable --now podman.socket</code></p>"},{"location":"contributing/tests/#running-the-test","title":"Running the test","text":"<p>Instruct Podman to communicate with the rootful Podman socket by setting the environment variable: <pre><code>CONTAINER_HOST=unix://run/podman/podman.sock make test-k8s\n</code></pre></p>"},{"location":"contributing/tests/#running-test-with-an-external-cluster","title":"Running test with an External Cluster","text":"<p>By default, the tests start a local cluster using kind. Instead, you can use an already existsing cluster.</p>"},{"location":"contributing/tests/#prerequisites","title":"Prerequisites","text":"<p>Since the test suite includes KubeVirt VMs, the cluster must include the <code>kubevirt</code> operator. See the Installation guide for details.</p>"},{"location":"contributing/tests/#kubeconfig","title":"Kubeconfig","text":"<p>Either save the kubeconfig file under <code>~/.kube/config</code> or set the environment variable <code>KUBECONFIG</code> to its location</p>"},{"location":"contributing/tests/#run-the-tests","title":"Run the tests","text":"<p>In order to instruct the tests to use the existing cluster set <code>USE_EXISTING_CLUSTER=yes</code> when calling <code>make</code>.</p>"},{"location":"contributing/tests/#execute-a-subset-of-the-tests","title":"Execute a subset of the tests","text":"<p>The list of executed tests may be filtered by setting the environment variable <code>TEST_FILTER</code>:</p> <pre><code>TEST_FILTER=\"datavolume\" make test-k8s\n</code></pre>"},{"location":"measurements/","title":"Measurements","text":"<p>Kube-burner allows you to get further metrics using other mechanisms or data sources, such as the Kubernetes API. These mechanisms are called measurements.</p> <p>Measurements are enabled in the <code>measurements</code> object of the configuration file. This object contains a list of measurements with their options.</p>"},{"location":"measurements/#pod-latency","title":"Pod latency","text":"<p>Collects latencies from the different pod startup phases, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: podLatency\n</code></pre>"},{"location":"measurements/#metrics","title":"Metrics","text":"<p>The metrics collected are pod latency timeseries (<code>podLatencyMeasurement</code>) and four documents holding a summary with different pod latency quantiles of each pod condition (<code>podLatencyQuantilesMeasurement</code>).</p> <p>One document, such as the following, is indexed per each pod created by the workload that enters in <code>Running</code> condition during the workload:</p> <pre><code>{\n  \"timestamp\": \"2020-11-15T20:28:59.598727718Z\",\n  \"schedulingLatency\": 4,\n  \"initializedLatency\": 20,\n  \"containersReadyLatency\": 2997,\n  \"podReadyLatency\": 2997,\n  \"metricName\": \"podLatencyMeasurement\",\n  \"uuid\": \"c40b4346-7af7-4c63-9ab4-aae7ccdd0616\",\n  \"namespace\": \"kubelet-density\",\n  \"podName\": \"kubelet-density-13\",\n  \"nodeName\": \"worker-001\",\n  \"jobName\": \"create-pods\",\n  \"jobIteration\": \"2\",\n  \"replica\": \"3\",\n}\n</code></pre> <p>Pod latency quantile sample:</p> <pre><code>{\n  \"quantileName\": \"Ready\",\n  \"uuid\": \"23c0b5fd-c17e-4326-a389-b3aebc774c82\",\n  \"P99\": 3774,\n  \"P95\": 3510,\n  \"P50\": 2897,\n  \"max\": 3774,\n  \"avg\": 2876.3,\n  \"timestamp\": \"2020-11-15T22:26:51.553221077+01:00\",\n  \"metricName\": \"podLatencyQuantilesMeasurement\",\n},\n{\n  \"quantileName\": \"PodScheduled\",\n  \"uuid\": \"23c0b5fd-c17e-4326-a389-b3aebc774c82\",\n  \"P99\": 64,\n  \"P95\": 8,\n  \"P50\": 5,\n  \"max\": 64,\n  \"avg\": 5.38,\n  \"timestamp\": \"2020-11-15T22:26:51.553225151+01:00\",\n  \"metricName\": \"podLatencyQuantilesMeasurement\",\n}\n</code></pre> <p>Where <code>quantileName</code> matches with the pod conditions and can be:</p> <ul> <li><code>PodScheduled</code>: Pod has been scheduled in to a node.</li> <li><code>PodReadyToStartContainers</code>: The Pod sandbox has been successfully created and networking configured.</li> <li><code>Initialized</code>: All init containers in the pod have started successfully</li> <li><code>ContainersReady</code>: Indicates whether all containers in the pod are ready.</li> <li><code>Ready</code>: The pod is able to service requests and should be added to the load balancing pools of all matching services.</li> </ul> <p>Note</p> <p>We also log the errorRate of the latencies for user's understanding. It indicates the percentage of pods out of all pods in the workload that got errored during the latency calculations. Currently the threshold for the errorRate is 10% and we do not log latencies if the error is &gt; 10% which indicates a problem with environment.(i.e system under test)</p> <p>Info</p> <p>More information about the pod conditions can be found at the kubernetes documentation site.</p> <p>And the metrics are:</p> <ul> <li><code>P99</code>: 99<sup>th</sup> percentile of the pod condition.</li> <li><code>P95</code>: 95<sup>th</sup> percentile of the pod condition.</li> <li><code>P50</code>: 50<sup>th</sup> percentile of the pod condition.</li> <li><code>Max</code>: Maximum value of the condition.</li> <li><code>Avg</code>: Average value of the condition.</li> </ul>"},{"location":"measurements/#pod-latency-thresholds","title":"Pod latency thresholds","text":"<p>It is possible to establish pod latency thresholds to the different pod conditions and metrics by defining the option <code>thresholds</code> within this measurement:</p> <p>Establishing a threshold of 2000ms in the P99 metric of the <code>Ready</code> condition.</p> <pre><code>  measurements:\n  - name: podLatency\n    thresholds:\n    - conditionType: Ready\n      metric: P99\n      threshold: 2000ms\n</code></pre> <p>Latency thresholds are evaluated at the end of each job, showing an informative message like the following:</p> <pre><code>INFO[2020-12-15 12:37:08] Evaluating latency thresholds\nWARN[2020-12-15 12:37:08] P99 Ready latency (2929ms) higher than configured threshold: 2000ms\n</code></pre> <p>In case of not meeting any of the configured thresholds, like the example above, kube-burner return code will be 1.</p>"},{"location":"measurements/#job-latency","title":"Job latency","text":"<p>Collects latencies from the different job stages, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: jobLatency\n</code></pre>"},{"location":"measurements/#metrics_1","title":"Metrics","text":"<p>The metrics collected are job latency timeseries (<code>jobLatencyMeasurement</code>) and two documents holding a summary of different job latency quantiles (<code>jobLatencyQuantilesMeasurement</code>).</p> <p>It generates a document like the following per each job:</p> <pre><code>  {\n    \"timestamp\": \"2025-05-07T10:31:03Z\",\n    \"startTimeLatency\": 0,\n    \"completionLatency\": 14000,\n    \"metricName\": \"jobLatencyMeasurement\",\n    \"uuid\": \"62d3c7a1-4faa-44fb-99ff-cf2b79cdfd22\",\n    \"jobName\": \"small\",\n    \"jobIteration\": 0,\n    \"replica\": 18,\n    \"namespace\": \"kueue-scale-s\",\n    \"k8sJobName\": \"kueue-scale-small-18\",\n    \"metadata\": {\n      \"ocpMajorVersion\": \"4.18\",\n      \"ocpVersion\": \"4.18.5\"\n    }\n  }\n</code></pre> <p>Where <code>completionLatency</code> and <code>starTimeLatency</code> indicate the job completion time and startup latency respectively since its creation timestamp.</p> <p>Job latency quantile sample:</p> <pre><code>[\n  {\n    \"quantileName\": \"StartTime\",\n    \"uuid\": \"62d3c7a1-4faa-44fb-99ff-cf2b79cdfd22\",\n    \"P99\": 0,\n    \"P95\": 0,\n    \"P50\": 0,\n    \"min\": 0,\n    \"max\": 0,\n    \"avg\": 0,\n    \"timestamp\": \"2025-05-07T10:31:22.181506023Z\",\n    \"metricName\": \"jobLatencyQuantilesMeasurement\",\n    \"jobName\": \"small\",\n    \"metadata\": {\n      \"ocpMajorVersion\": \"4.18\",\n      \"ocpVersion\": \"4.18.5\"\n    }\n  },\n  {\n    \"quantileName\": \"Complete\",\n    \"uuid\": \"62d3c7a1-4faa-44fb-99ff-cf2b79cdfd22\",\n    \"P99\": 15000,\n    \"P95\": 15000,\n    \"P50\": 14000,\n    \"min\": 13000,\n    \"max\": 15000,\n    \"avg\": 13950,\n    \"timestamp\": \"2025-05-07T10:31:22.181509954Z\",\n    \"metricName\": \"jobLatencyQuantilesMeasurement\",\n    \"jobName\": \"small\",\n    \"metadata\": {\n      \"ocpMajorVersion\": \"4.18\",\n      \"ocpVersion\": \"4.18.5\"\n    }\n  }\n]\n</code></pre>"},{"location":"measurements/#vmi-latency","title":"VMI latency","text":"<p>Collects latencies from the different vm/vmi startup phases, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: vmiLatency\n</code></pre>"},{"location":"measurements/#metrics_2","title":"Metrics","text":"<p>The metrics collected are vm/vmi latency timeseries (<code>vmiLatencyMeasurement</code>) and up to 10 documents holding a summary with different vm/vmi latency quantiles of each condition (<code>vmiLatencyQuantilesMeasurement</code>).</p> <p>One document, such as the following, is indexed per each vm/vmi created by the workload that enters in <code>Running</code> condition during the workload:</p> <pre><code>{\n    \"timestamp\": \"2024-11-26T12:57:50Z\",\n    \"podCreatedLatency\": 116532,\n    \"podScheduledLatency\": 116574,\n    \"podInitializedLatency\": 132511,\n    \"podContainersReadyLatency\": 135541,\n    \"podReadyLatency\": 135541,\n    \"vmiCreatedLatency\": 7000,\n    \"vmiPendingLatency\": 116401,\n    \"vmiSchedulingLatency\": 117106,\n    \"vmiScheduledLatency\": 127926,\n    \"vmiRunningLatency\": 138166,\n    \"vmReadyLatency\": 138166,\n    \"metricName\": \"vmiLatencyMeasurement\",\n    \"uuid\": \"f7c79fd5-58e7-4719-a710-7633ffb20491\",\n    \"namespace\": \"virt-density\",\n    \"podName\": \"virt-launcher-virt-density-27-zkfdt\",\n    \"vmName\": \"virt-density-27\",\n    \"vmiName\": \"virt-density-27\",\n    \"nodeName\": \"y37-h25-000-r740xd\",\n    \"jobName\": \"virt-density\",\n}\n</code></pre> <p>Info</p> <p>The fields <code>vmReadyLatency</code> and <code>vmName</code> are only set when the VMI has a parent VM object</p> <p>Info</p> <p>The fields prefixed by <code>pod</code>, represent the latency of the different startup phases of the pod running the actual virtual machine.</p> <p>Pod latency quantile sample:</p> <pre><code>[\n  {\n    \"quantileName\": \"PodPodScheduled\",\n    \"uuid\": \"f7c79fd5-58e7-4719-a710-7633ffb20491\",\n    \"P99\": 125183,\n    \"P95\": 124870,\n    \"P50\": 119769,\n    \"max\": 125313,\n    \"avg\": 119509,\n    \"timestamp\": \"2024-11-26T13:00:30.713169517Z\",\n    \"metricName\": \"vmiLatencyQuantilesMeasurement\",\n    \"jobName\": \"virt-density\",\n  },\n  {\n    \"quantileName\": \"VMIScheduled\",\n    \"uuid\": \"f7c79fd5-58e7-4719-a710-7633ffb20491\",\n    \"P99\": 144029,\n    \"P95\": 143255,\n    \"P50\": 139240,\n    \"max\": 144029,\n    \"avg\": 138699,\n    \"timestamp\": \"2024-11-26T13:00:30.713173817Z\",\n    \"metricName\": \"vmiLatencyQuantilesMeasurement\",\n    \"jobName\": \"virt-density\",\n  },\n  {\n    \"quantileName\": \"PodContainersReady\",\n    \"uuid\": \"f7c79fd5-58e7-4719-a710-7633ffb20491\",\n    \"P99\": 144029,\n    \"P95\": 143257,\n    \"P50\": 139466,\n    \"max\": 144030,\n    \"avg\": 139361,\n    \"timestamp\": \"2024-11-26T13:00:30.713179584Z\",\n    \"metricName\": \"vmiLatencyQuantilesMeasurement\",\n    \"jobName\": \"virt-density\",\n  },\n]\n</code></pre>"},{"location":"measurements/#node-latency","title":"Node latency","text":"<p>Collects latencies from the different node conditions on the cluster, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: nodeLatency\n</code></pre>"},{"location":"measurements/#metrics_3","title":"Metrics","text":"<p>The metrics collected are node latency timeseries (<code>nodeLatencyMeasurement</code>) and four documents holding a summary with different node latency quantiles of each node condition (<code>nodeLatencyQuantilesMeasurement</code>).</p> <p>One document, such as the following, is indexed per each node created by the workload that enters in <code>Ready</code> condition during the workload:</p> <pre><code>{\n  \"timestamp\": \"2024-08-25T12:49:26Z\",\n  \"nodeMemoryPressureLatency\": 0,\n  \"nodeDiskPressureLatency\": 0,\n  \"nodePIDPressureLatency\": 0,\n  \"nodeReadyLatency\": 82000,\n  \"metricName\": \"nodeLatencyMeasurement\",\n  \"uuid\": \"4f9e462c-cacc-4695-95db-adfb841e0980\",\n  \"jobName\": \"namespaced\",\n  \"nodeName\": \"ip-10-0-34-104.us-west-2.compute.internal\",\n  \"labels\": {\n    \"beta.kubernetes.io/arch\": \"amd64\",\n    \"beta.kubernetes.io/instance-type\": \"m6i.xlarge\",\n    \"beta.kubernetes.io/os\": \"linux\",\n    \"failure-domain.beta.kubernetes.io/region\": \"us-west-2\",\n    \"failure-domain.beta.kubernetes.io/zone\": \"us-west-2b\",\n    \"kubernetes.io/arch\": \"amd64\",\n    \"kubernetes.io/hostname\": \"ip-10-0-34-104.us-west-2.compute.internal\",\n    \"kubernetes.io/os\": \"linux\",\n    \"node-role.kubernetes.io/worker\": \"\",\n    \"node.kubernetes.io/instance-type\": \"m6i.xlarge\",\n    \"node.openshift.io/os_id\": \"rhcos\",\n    \"topology.ebs.csi.aws.com/zone\": \"us-west-2b\",\n    \"topology.kubernetes.io/region\": \"us-west-2\",\n    \"topology.kubernetes.io/zone\": \"us-west-2b\"\n  }\n}\n</code></pre> <p>Node latency quantile sample:</p> <pre><code>{\n  \"quantileName\": \"Ready\",\n  \"uuid\": \"4f9e462c-cacc-4695-95db-adfb841e0980\",\n  \"P99\": 163000,\n  \"P95\": 163000,\n  \"P50\": 93000,\n  \"max\": 163000,\n  \"avg\": 122500,\n  \"timestamp\": \"2024-08-25T20:42:59.422208263Z\",\n  \"metricName\": \"nodeLatencyQuantilesMeasurement\",\n  \"jobName\": \"namespaced\",\n  \"metadata\": {}\n},\n{\n  \"quantileName\": \"MemoryPressure\",\n  \"uuid\": \"4f9e462c-cacc-4695-95db-adfb841e0980\",\n  \"P99\": 0,\n  \"P95\": 0,\n  \"P50\": 0,\n  \"max\": 0,\n  \"avg\": 0,\n  \"timestamp\": \"2024-08-25T20:42:59.422209628Z\",\n  \"metricName\": \"nodeLatencyQuantilesMeasurement\",\n  \"jobName\": \"namespaced\",\n  \"metadata\": {}\n}\n</code></pre> <p>Where <code>quantileName</code> matches with the node conditions and can be:</p> <ul> <li><code>MemoryPressure</code>: Indicates if pressure exists on node memory.</li> <li><code>DiskPressure</code>: Indicates if pressure exists on node size.</li> <li><code>PIDPressure</code>: Indicates if pressure exists because of too many processes.</li> <li><code>Ready</code>: Node is ready and able to accept pods.</li> </ul> <p>Info</p> <p>More information about the node conditions can be found at the kubernetes documentation site.</p> <p>And the metrics, error rates, and their thresholds work the same way as in the pod latency measurement.</p>"},{"location":"measurements/#pvc-latency","title":"PVC latency","text":"<p>Note: This measurement is not supported for patch, read and delete jobs. Because it requires all the events from creation to reaching a stable end state to happen during a job.</p> <p>Collects latencies from different pvc phases on the cluster, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: pvcLatency\n</code></pre>"},{"location":"measurements/#metrics_4","title":"Metrics","text":"<p>The metrics collected are pvc latency timeseries (<code>pvcLatencyMeasurement</code>) and 2-3 documents holding a summary with different pvc latency quantiles of each lifecycle phase (<code>pvcLatencyQuantilesMeasurement</code>).</p> <p>One document, such as the following, is indexed per each pvc created by the workload that enters in <code>Bound/Lost</code> condition during the workload:</p> <pre><code>{\n  \"timestamp\": \"2025-01-10T02:50:50.247528962Z\",\n  \"pendingLatency\": 37,\n  \"bindingLatency\": 4444,\n  \"lostLatency\": 0,\n  \"uuid\": \"1f16ffd1-ac65-47c4-970f-a71d5f309cf5\",\n  \"pvcName\": \"deployment-pvc-move-1\",\n  \"jobName\": \"pvc-move\",\n  \"namespace\": \"deployment-pvc-move-0\",\n  \"metricName\": \"pvcLatencyMeasurement\",\n  \"size\": \"1Gi\",\n  \"storageClass\": \"gp3-csi\",\n  \"jobIteration\": 0,\n  \"replica\": 1,\n}\n</code></pre> <p>PVC latency quantile sample:</p> <pre><code>[\n  {\n    \"quantileName\": \"Bound\",\n    \"uuid\": \"1f16ffd1-ac65-47c4-970f-a71d5f309cf5\",\n    \"P99\": 4444,\n    \"P95\": 4444,\n    \"P50\": 4444,\n    \"min\": 4444,\n    \"max\": 4444,\n    \"avg\": 4444,\n    \"timestamp\": \"2025-01-10T02:51:04.611059008Z\",\n    \"metricName\": \"pvcLatencyQuantilesMeasurement\",\n    \"jobName\": \"pvc-move\",\n  },\n  {\n    \"quantileName\": \"Lost\",\n    \"uuid\": \"1f16ffd1-ac65-47c4-970f-a71d5f309cf5\",\n    \"P99\": 0,\n    \"P95\": 0,\n    \"P50\": 0,\n    \"min\": 0,\n    \"max\": 0,\n    \"avg\": 0,\n    \"timestamp\": \"2025-01-10T02:51:04.611061474Z\",\n    \"metricName\": \"pvcLatencyQuantilesMeasurement\",\n    \"jobName\": \"pvc-move\",\n  },\n  {\n    \"quantileName\": \"Pending\",\n    \"uuid\": \"1f16ffd1-ac65-47c4-970f-a71d5f309cf5\",\n    \"P99\": 37,\n    \"P95\": 37,\n    \"P50\": 37,\n    \"min\": 37,\n    \"max\": 37,\n    \"avg\": 37,\n    \"timestamp\": \"2025-01-10T02:51:04.611062824Z\",\n    \"metricName\": \"pvcLatencyQuantilesMeasurement\",\n    \"jobName\": \"pvc-move\",\n  }\n]\n</code></pre> <p>Where <code>quantileName</code> matches with the pvc phases and can be:</p> <ul> <li><code>Pending</code>: Indicates that PVC is not yet bound.</li> <li><code>Bound</code>: Indicates that PVC is bound.</li> <li><code>Lost</code>: Indicates that the PVC has lost their underlying PersistentVolume.</li> </ul> <p>Info</p> <p>More information about the PVC phases can be found at the kubernetes api documentation.</p> <p>And the metrics, error rates, and their thresholds work the same way as in the other latency measurements.</p>"},{"location":"measurements/#service-latency","title":"Service latency","text":"<p>Calculates the time taken the services to serve requests once their endpoints are ready. This measurement works as follows.</p> <pre><code>graph LR\n    A[Service created] --&gt; C{active endpoints?}\n    C --&gt;|No| C\n    C --&gt;|Yes| D[Save timestamp]\n    D --&gt; G{TCP connectivity?}\n    G--&gt;|Yes| F(Generate metric)\n    G --&gt;|No| G</code></pre> <p>Where the service latency is the time elapsed since the service has at least one endpoint ready till the connectivity is verified.</p> <p>The connectivity check is done through a pod running in the <code>kube-burner-service-latency</code> namespace, kube-burner connects to this pod and uses <code>netcat</code> to verify connectivity.</p> <p>This measure is enabled with:</p> <pre><code>  measurements:\n  - name: serviceLatency\n    svcTimeout: 5s\n</code></pre> <p>Where <code>svcTimeout</code>, by default <code>5s</code>, defines the maximum amount of time the measurement will wait for a service to be ready, when this timeout is met, the metric from that service is discarded.</p> <p>Considerations</p> <ul> <li>Only TCP is supported.</li> <li>Supported services are <code>ClusterIP</code>, <code>NodePort</code> and <code>LoadBalancer</code>.</li> <li>kube-burner starts checking service connectivity when its endpoints object has at least one address.</li> <li>Make sure the endpoints of the service are correct and reachable from the pod running in the <code>kube-burner-service-latency</code>.</li> <li>When the service is <code>NodePort</code>, the connectivity check is done against the node where the connectivity check pods runs.</li> <li>By default all services created by the benchmark are tracked by this measurement, it's possible to discard service objects from tracking by annotating them with <code>kube-burner.io/service-latency=false</code>.</li> <li>Keep in mind that When service is <code>LoadBalancer</code> type, the provider needs to setup the load balancer, which adds some extra delay.</li> <li>Endpoints are pinged one after another, this can create some delay when the number of endpoints of the service is big.</li> </ul>"},{"location":"measurements/#metrics_5","title":"Metrics","text":"<p>The metrics collected are service latency timeseries (<code>svcLatencyMeasurement</code>) and another document that holds a summary with the different service latency quantiles (<code>svcLatencyQuantilesMeasurement</code>). It is possible to skip indexing the <code>svcLatencyMeasurement</code> metric by configuring the field <code>svcLatencyMetrics</code> of this measurement to <code>quantiles</code>. Metric documents have the following structure:</p> <pre><code>{\n  \"timestamp\": \"2023-11-19T00:41:51Z\",\n  \"ready\": 1631880721,\n  \"metricName\": \"svcLatencyMeasurement\",\n  \"uuid\": \"c4558ba8-1e29-4660-9b31-02b9f01c29bf\",\n  \"namespace\": \"cluster-density-v2-2\",\n  \"service\": \"cluster-density-1\",\n  \"type\": \"ClusterIP\"\n}\n</code></pre> <p>Note</p> <p>When type is <code>LoadBalancer</code>, it includes an extra field <code>ipAssigned</code>, that reports the IP assignation latency of the service.</p> <p>And the quantiles document has the structure:</p> <pre><code>{\n  \"quantileName\": \"Ready\",\n  \"uuid\": \"c4558ba8-1e29-4660-9b31-02b9f01c29bf\",\n  \"P99\": 1867593282,\n  \"P95\": 1856488440,\n  \"P50\": 1723817691,\n  \"max\": 1868307027,\n  \"avg\": 1722308938,\n  \"timestamp\": \"2023-11-19T00:42:26.663991359Z\",\n  \"metricName\": \"svcLatencyQuantilesMeasurement\",\n},\n{\n  \"quantileName\": \"LoadBalancer\",\n  \"uuid\": \"c4558ba8-1e29-4660-9b31-02b9f01c29bf\",\n  \"P99\": 1467593282,\n  \"P95\": 1356488440,\n  \"P50\": 1323817691,\n  \"max\": 2168307027,\n  \"avg\": 1822308938,\n  \"timestamp\": \"2023-11-19T00:42:26.663991359Z\",\n  \"metricName\": \"svcLatencyQuantilesMeasurement\",\n}\n</code></pre> <p>When there're <code>LoadBalancer</code> services, an extra document with <code>quantileName</code> as <code>LoadBalancer</code> is also generated as shown above.</p>"},{"location":"measurements/#datavolume-latency","title":"DataVolume Latency","text":"<p>Collects latencies from different DataVolume phases on the cluster, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: dataVolumeLatency\n</code></pre>"},{"location":"measurements/#metrics_6","title":"Metrics","text":"<p>The metrics collected are data volume latency timeseries (<code>dataVolumeLatencyMeasurement</code>) and 2-3 documents holding a summary with different data volume latency quantiles of each lifecycle phase (<code>dataVolumeLatencyQuantilesMeasurement</code>).</p> <p>One document, such as the following, is indexed per each data volume created by the workload that enters in <code>Ready</code> condition during the workload:</p> <pre><code>{\n  \"timestamp\": \"2025-01-13T14:55:44Z\",\n  \"dvBoundLatency\": 8000,\n  \"dvRunningLatency\": 0,\n  \"dvReadyLatency\": 8000,\n  \"metricName\": \"dvLatencyMeasurement\",\n  \"uuid\": \"ba6afa06-d780-4306-b97e-bfcce60fb5a7\",\n  \"namespace\": \"catalog\",\n  \"dvName\": \"master-image\",\n  \"jobName\": \"create-base-image-dv\",\n  \"jobIteration\": 0,\n  \"replica\": 1,\n}\n</code></pre> <p>DataVolume latency quantile sample:</p> <pre><code>[\n  {\n    \"quantileName\": \"Bound\",\n    \"uuid\": \"59b14eb2-339a-4761-8593-195eb80943a9\",\n    \"P99\": 39000,\n    \"P95\": 39000,\n    \"P50\": 19000,\n    \"min\": 4000,\n    \"max\": 42000,\n    \"avg\": 21900,\n    \"timestamp\": \"2025-01-14T14:40:15.3046Z\",\n    \"metricName\": \"dvLatencyQuantilesMeasurement\",\n    \"jobName\": \"create-vms\",\n  },\n  {\n    \"quantileName\": \"Running\",\n    \"uuid\": \"59b14eb2-339a-4761-8593-195eb80943a9\",\n    \"P99\": 3000,\n    \"P95\": 3000,\n    \"P50\": 2000,\n    \"min\": 2000,\n    \"max\": 3000,\n    \"avg\": 2000,\n    \"timestamp\": \"2025-01-14T14:40:15.304602Z\",\n    \"metricName\": \"dvLatencyQuantilesMeasurement\",\n    \"jobName\": \"create-vms\",\n  },\n  {\n    \"quantileName\": \"Ready\",\n    \"uuid\": \"59b14eb2-339a-4761-8593-195eb80943a9\",\n    \"P99\": 39000,\n    \"P95\": 39000,\n    \"P50\": 19000,\n    \"min\": 4000,\n    \"max\": 42000,\n    \"avg\": 22000,\n    \"timestamp\": \"2025-01-14T14:40:15.304604Z\",\n    \"metricName\": \"dvLatencyQuantilesMeasurement\",\n    \"jobName\": \"create-vms\",\n  }\n]\n</code></pre> <p>Where <code>quantileName</code> matches with the pvc phases and can be:</p> <ul> <li><code>Running</code>: Indicates that DV is running and being populated if needed</li> <li><code>Bound</code>: Indicates that DV is bound</li> <li><code>Ready</code>: Indicates that the DV is ready for usage</li> </ul> <p>Info</p> <p>More information about the DataVolume condition types can be found at the kubevirt documentation.</p> <p>And the metrics, error rates, and their thresholds work the same way as in the other latency measurements.</p>"},{"location":"measurements/#volumesnapshot-latency","title":"VolumeSnapshot Latency","text":"<p>Collects latencies from different VolumeSnapshot phases on the cluster, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: volumeSnapshotLatency\n</code></pre>"},{"location":"measurements/#metrics_7","title":"Metrics","text":"<p>The metrics collected are data volume latency timeseries (<code>volumeSnapshotLatencyMeasurement</code>) and 2-3 documents holding a summary with different Volume Snapshot latency quantiles of each lifecycle phase (<code>volumeSnapshotLatencyQuantilesMeasurement</code>).</p> <p>One document, such as the following, is indexed per each Volume Snapshot created by the workload that enters in <code>Ready</code> condition during the workload:</p> <pre><code>{\n  \"timestamp\": \"2025-01-13T14:55:44Z\",\n  \"vsReadyLatency\": 8000,\n  \"metricName\": \"volumeSnapshotLatencyMeasurement\",\n  \"uuid\": \"ba6afa06-d780-4306-b97e-bfcce60fb5a7\",\n  \"namespace\": \"catalog\",\n  \"vsName\": \"master-image\",\n  \"jobName\": \"create-base-image-snapshot\",\n  \"jobIteration\": 0,\n  \"replica\": 1,\n}\n</code></pre> <p>VolumeSnapshot latency quantile sample:</p> <pre><code>[\n  {\n    \"quantileName\": \"Ready\",\n    \"uuid\": \"59b14eb2-339a-4761-8593-195eb80943a9\",\n    \"P99\": 39000,\n    \"P95\": 39000,\n    \"P50\": 19000,\n    \"min\": 4000,\n    \"max\": 42000,\n    \"avg\": 22000,\n    \"timestamp\": \"2025-01-14T14:40:15.304604Z\",\n    \"metricName\": \"volumeSnapshotLatencyQuantilesMeasurement\",\n    \"jobName\": \"create-snapshots\",\n  }\n]\n</code></pre> <p>Where <code>quantileName</code> matches with the pvc phases and can be:</p> <ul> <li><code>Ready</code>: Indicates that the Volume Snapshot is ready for usage</li> </ul> <p>And the metrics, error rates, and their thresholds work the same way as in the other latency measurements.</p>"},{"location":"measurements/#virtualmachineinstancemigration-latency","title":"VirtualMachineInstanceMigration Latency","text":"<p>Collects latencies from different VirtualMachineInstanceMigration phases on the cluster, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: vmimLatency\n</code></pre>"},{"location":"measurements/#metrics_8","title":"Metrics","text":"<p>The metrics collected are VirtualMachineInstanceMigration latency timeseries (<code>vmimLatencyMeasurement</code>) and 2-3 documents holding a summary with different VirtualMachineInstanceMigration latency quantiles of each lifecycle phase (<code>vmimLatencyQuantilesMeasurement</code>).</p> <p>One document, such as the following, is indexed per each VirtualMachineInstanceMigration created by the workload that enters in <code>Succeeded</code> phase during the workload:</p> <pre><code>{\n  \"timestamp\": \"2025-07-11T19:46:01Z\",\n  \"pendingLatency\": 0,\n  \"schedulingLatency\": 3000,\n  \"scheduledLatency\": 10000,\n  \"preparingTargetLatency\": 10000,\n  \"targetReadyLatency\": 12000,\n  \"runningLatency\": 12000,\n  \"succeededLatency\": 14000,\n  \"metricName\": \"vmimLatencyMeasurement\",\n  \"uuid\": \"3d655527-2562-4931-b40f-61adafbf49b0\",\n  \"namespace\": \"kubevirt-ops-0\",\n  \"vmimName\": \"kubevirt-ops-1-vm-op-1-k829h\",\n  \"vmiName\": \"kubevirt-ops-1\",\n  \"jobName\": \"vm-op\",\n  \"jobIteration\": 1,\n  \"replica\": 0\n}\n</code></pre> <p>VirtualMachineInstanceMigration latency quantile sample:</p> <pre><code>[\n  {\n    \"quantileName\": \"Pending\",\n    \"uuid\": \"3d655527-2562-4931-b40f-61adafbf49b0\",\n    \"P99\": 0,\n    \"P95\": 0,\n    \"P50\": 0,\n    \"min\": 0,\n    \"max\": 0,\n    \"avg\": 0,\n    \"timestamp\": \"2025-07-11T19:46:31.968793Z\",\n    \"metricName\": \"vmimLatencyQuantilesMeasurement\",\n    \"jobName\": \"vm-op\",\n    \"metadata\": null\n  },\n  {\n    \"quantileName\": \"Scheduling\",\n    \"uuid\": \"3d655527-2562-4931-b40f-61adafbf49b0\",\n    \"P99\": 1500,\n    \"P95\": 1500,\n    \"P50\": 0,\n    \"min\": 0,\n    \"max\": 3000,\n    \"avg\": 1500,\n    \"timestamp\": \"2025-07-11T19:46:31.968856Z\",\n    \"metricName\": \"vmimLatencyQuantilesMeasurement\",\n    \"jobName\": \"vm-op\",\n    \"metadata\": null\n  },\n  {\n    \"quantileName\": \"Scheduled\",\n    \"uuid\": \"3d655527-2562-4931-b40f-61adafbf49b0\",\n    \"P99\": 8000,\n    \"P95\": 8000,\n    \"P50\": 6000,\n    \"min\": 6000,\n    \"max\": 10000,\n    \"avg\": 8000,\n    \"timestamp\": \"2025-07-11T19:46:31.968861Z\",\n    \"metricName\": \"vmimLatencyQuantilesMeasurement\",\n    \"jobName\": \"vm-op\",\n    \"metadata\": null\n  },\n  {\n    \"quantileName\": \"PreparingTarget\",\n    \"uuid\": \"3d655527-2562-4931-b40f-61adafbf49b0\",\n    \"P99\": 8000,\n    \"P95\": 8000,\n    \"P50\": 6000,\n    \"min\": 6000,\n    \"max\": 10000,\n    \"avg\": 8000,\n    \"timestamp\": \"2025-07-11T19:46:31.968864Z\",\n    \"metricName\": \"vmimLatencyQuantilesMeasurement\",\n    \"jobName\": \"vm-op\",\n    \"metadata\": null\n  },\n  {\n    \"quantileName\": \"TargetReady\",\n    \"uuid\": \"3d655527-2562-4931-b40f-61adafbf49b0\",\n    \"P99\": 10000,\n    \"P95\": 10000,\n    \"P50\": 8000,\n    \"min\": 8000,\n    \"max\": 12000,\n    \"avg\": 10000,\n    \"timestamp\": \"2025-07-11T19:46:31.968868Z\",\n    \"metricName\": \"vmimLatencyQuantilesMeasurement\",\n    \"jobName\": \"vm-op\",\n    \"metadata\": null\n  },\n  {\n    \"quantileName\": \"Running\",\n    \"uuid\": \"3d655527-2562-4931-b40f-61adafbf49b0\",\n    \"P99\": 10000,\n    \"P95\": 10000,\n    \"P50\": 8000,\n    \"min\": 8000,\n    \"max\": 12000,\n    \"avg\": 10000,\n    \"timestamp\": \"2025-07-11T19:46:31.968872Z\",\n    \"metricName\": \"vmimLatencyQuantilesMeasurement\",\n    \"jobName\": \"vm-op\",\n    \"metadata\": null\n  },\n  {\n    \"quantileName\": \"Succeeded\",\n    \"uuid\": \"3d655527-2562-4931-b40f-61adafbf49b0\",\n    \"P99\": 12000,\n    \"P95\": 12000,\n    \"P50\": 10000,\n    \"min\": 10000,\n    \"max\": 14000,\n    \"avg\": 12000,\n    \"timestamp\": \"2025-07-11T19:46:31.968875Z\",\n    \"metricName\": \"vmimLatencyQuantilesMeasurement\",\n    \"jobName\": \"vm-op\",\n    \"metadata\": null\n  }\n]\n</code></pre> <p>Where <code>quantileName</code> matches with the VirtualMachineInstance phases and can be:</p> <ul> <li><code>Pending</code>: The migration is accepted by the system</li> <li><code>Scheduling</code> The migration's target pod is being scheduled</li> <li><code>Scheduled</code>: The migration's target pod is running</li> <li><code>PreparingTarget</code>: The migration's target pod is being prepared for migration</li> <li><code>TargetReady</code>: The migration's target pod is prepared and ready for migration</li> <li><code>Running</code>: The migration is in progress</li> <li><code>Succeeded</code>: The migration passed</li> </ul> <p>And the metrics, error rates, and their thresholds work the same way as in the other latency measurements.</p>"},{"location":"measurements/#network-policy-latency","title":"Network Policy Latency","text":"<p>Note: This measurement has requirement of having 2 jobs defined in the templates. It doesn't report the network policy latency measurement if only one job is used.</p> <p>Calculates the time taken to apply the network policy rules by the SDN through connection testing between pods specified in the network policy. At a high level, Kube-burner utilizes a proxy pod, <code>network-policy-proxy</code>, to distribute connection information (such as remote IP addresses) to the client pods. These client pods then send the requests to the specified addresses and record a timestamp once a connection is successfully established. Kube-burner retrieves these timestamps from the client pods via the proxy pod and calculates the network policy latency by comparing the recorded timestamp with the timestamp when the network policy was created.</p> <p>During this testing, kube-burner creates two separate jobs: 1. Job 1: Creates all necessary namespaces and pods. 2. Job 2: Applies network policies to the pods defined on above namespaces and tests connections by sending HTTP requests.</p> <p>Latency measurement is only performed during the execution of Job 2, which focuses on network policy application and connection testing.</p>"},{"location":"measurements/#templates","title":"Templates","text":"<p>Use the examples/workloads/network-policy/network-policy.yml for reference.</p>"},{"location":"measurements/#dependency","title":"Dependency","text":"<ol> <li>2 Jobs should be used in the template</li> <li>Both the jobs should use the same namespace name, example \"network-policy-perf\".</li> <li>First job should create the pods. Second job should create the network policies</li> <li>Label \"kube-burner.io/skip-networkpolicy-latency: true\" should be defined under namespaceLabels.</li> <li>All the pods should allow traffic from the proxy pod. Example workload uses np-deny-all.yml and np-allow-from-proxy.yml object templates for this.</li> <li>inputVars.namespaces for templates/ingress-np.yml object template should have same value as jobIterations</li> </ol>"},{"location":"measurements/#sequence-of-events-during-connection-testing","title":"Sequence of Events During Connection Testing:","text":"<ol> <li> <p>Proxy Pod Initialization:    Kube-burner internally creates the <code>network-policy-proxy</code> pod and uses port forwarding for communication.</p> </li> <li> <p>Job Execution:</p> </li> <li>Job 1: Kube-burner creates namespaces and pods as per the configuration.</li> <li>Job 2: Kube-burner applies network policies and runs connection tests as follows:</li> </ol>"},{"location":"measurements/#reason-for-using-2-jobs-for-the-testing","title":"Reason for using 2 jobs for the testing:","text":"<p>Network policy is not applied only between the local pods. Instead, the network policy create rules between the local pods and remote pods which are hosted on remote namespaces. For example, assume if the job is creating 50 ingress and 50 egress network policies per namespace and we have 240 namespaces. In the first namespace, i.e network-policy-perf-0, network policy ingress-0-29 allows traffic to the local pods in the current namespace (i.e network-policy-perf-0.) from pods in remote namespaces network-policy-perf-41, network-policy-perf-42, network-policy-perf-43, network-policy-perf-44 and network-policy-perf-41.</p> <p>In the current implementation, pods which are already created in \"network-policy-perf-41, network-policy-perf-42, network-policy-perf-43, network-policy-perf-44 and network-policy-perf-41\" try to send http requests to pods in \"network-policy-perf-0\" even before the network policy \"ingress-0-29\" in network-policy-perf-0 created. But these http requests will be failing as the network policy is not yet created. Once the \"ingress-0-29\" network policies created and ovs flows get programmed, the http requests will be succesful.</p> <p>So this approach is helpful in testing connections between pods of different namespaces. Here all other resources (namespaces, pods, client app with peer addresses to send http requests) are ready and the dependency is only on the network policy creation to make the connection succesful.</p> <p>Also as the pods are already created, this pod latency doesn't impact the network policy latency. For example when kube-burner reports 6 seconds as network policy readiness latency, it is purely network policy creation and resulting ovs flows and not any pod readiness latency.</p> <p>On a summary, the advantages with this approach are -</p> <ol> <li>OVN components are dedicated for only network policy processing</li> <li>CPU &amp; memory usage metrics captured are isolated for only network policy creation</li> <li>Network policy latency calculation doesn\u2019t include pod readiness as pods are already existing</li> <li>network policy rules are applied between pods across namespaces</li> </ol>"},{"location":"measurements/#steps-in-the-execution-of-job-2","title":"Steps in the Execution of Job 2:","text":"<ol> <li>Connection List Preparation:</li> <li>Kube-burner parses the network policy template to prepare a list of connection details for each client pod. Each connection is defined by a remote IP address, port, and the associated network policy name.</li> <li>Currently, connection testing supports only the Ingress rule on port 8080.</li> <li> <p>Although the template may specify identical configurations for multiple network policies within a namespace, Kube-burner optimizes this process by ensuring that only unique connections are sent to the client pods (avoiding duplicate remote IP addresses).</p> </li> <li> <p>Sending Connection Information:</p> </li> <li> <p>Kube-burner passes the prepared connection information to the client pods via the <code>network-policy-proxy</code> pod and waits until the proxy pod confirms that all client pods have received the information.</p> </li> <li> <p>Initial HTTP Requests:</p> </li> <li> <p>Once the client pods receive their connection details, they begin sending HTTP requests to the specified addresses. Initially, these requests will fail because the network policies have not yet been applied.</p> </li> <li> <p>Network Policy Creation:</p> </li> <li>As part of its regular workflow, Kube-burner parses the template and applies network policies for each namespace.</li> <li> <p>After the network policies are applied, the previously failing HTTP requests from the client pods become successful. At this point, each client pod records the timestamp when a successful connection is established.</p> </li> <li> <p>Pause for Stabilization:</p> </li> <li> <p>After creating all network policies, Kube-burner pauses for 1 minute (due to the <code>jobPause: 1m</code> configuration option in the template). This allows the connection tests to complete successfully within this time window.</p> </li> <li> <p>Retrieving Timestamps and Calculating Latency:</p> </li> <li>Kube-burner retrieves the recorded connection timestamps from the client pods via the proxy pod.</li> <li>The latency is then calculated by comparing the recorded connection timestamp with the timestamp when the network policy was applied. This value represents the time taken for the SDN to enforce network policies. A network policy, when applied, tests connection between multiple client and server pods and measure the latency of each connection. We report max and min values of these connections latencies for a network policy.</li> </ol> <p>This measure is enabled with:</p> <pre><code>  measurements:\n    - name: netpolLatency\n</code></pre>"},{"location":"measurements/#metrics_9","title":"Metrics","text":"<p>And the quantiles document has the structure: <pre><code>[\n  {\n    \"quantileName\": \"Ready\",\n    \"uuid\": \"734adc28-c5b4-4a69-8807-ff99195bca1b\",\n    \"P99\": 3040,\n    \"P95\": 3038,\n    \"P50\": 2077,\n    \"max\": 3040,\n    \"avg\": 2264,\n    \"timestamp\": \"2024-10-10T10:39:29.059701196Z\",\n    \"metricName\": \"netpolLatencyQuantilesMeasurement\",\n    \"jobName\": \"network-policy-perf\",\n    \"metadata\": {}\n  },\n  {\n    \"quantileName\": \"minReady\",\n    \"uuid\": \"734adc28-c5b4-4a69-8807-ff99195bca1b\",\n    \"P99\": 3025,\n    \"P95\": 3024,\n    \"P50\": 2058,\n    \"max\": 3025,\n    \"avg\": 2245,\n    \"timestamp\": \"2024-10-10T10:39:29.059703401Z\",\n    \"metricName\": \"netpolLatencyQuantilesMeasurement\",\n    \"jobName\": \"network-policy-perf\",\n    \"metadata\": {}\n  }\n]\n</code></pre></p>"},{"location":"measurements/#pprof-collection","title":"pprof collection","text":"<p>This measurement can be used to collect Golang profiling information from processes running in pods from the cluster. To do so, kube-burner connects to pods labeled with <code>labelSelector</code> and running in <code>namespace</code>. This measurement uses an implementation similar to <code>kubectl exec</code>, and as soon as it connects to one pod it executes the command <code>curl &lt;pprofURL&gt;</code> to get the pprof data. pprof files are collected in a regular basis configured by the parameter <code>pprofInterval</code>, the collected pprof files are downloaded from the pods to the local directory configured by the parameter <code>pprofDirectory</code> which by default is <code>pprof</code>.</p> <p>As some components require authentication to get profiling information, <code>kube-burner</code> provides two different modalities to address it:</p> <ul> <li>Bearer token authentication: This modality is configured by the variable <code>bearerToken</code>, which holds a valid Bearer token that will be used by cURL to get pprof data. This method is usually valid with kube-apiserver and kube-controller-managers components</li> <li>Certificate Authentication: Usually valid for etcd, this method can be configured using a combination of cert/privKey files or directly using the cert/privkey content, it can be tweaked with the following variables:<ul> <li><code>cert</code>: Base64 encoded certificate.</li> <li><code>key</code>: Base64 encoded private key.</li> <li><code>certFile</code>: Path to a certificate file.</li> <li><code>keyFile</code>: Path to a private key file.</li> </ul> </li> </ul> <p>Note</p> <p>The decoded content of the certificate and private key is written to the files /tmp/pprof.crt and /tmp/pprof.key of the remote pods respectively</p> <p>An example of how to configure this measurement to collect pprof HEAP and CPU profiling data from kube-apiserver and etcd is shown below:</p> <pre><code>  measurements:\n  - name: pprof\n    pprofInterval: 30m\n    pprofDirectory: pprof-data\n    pprofTargets:\n    - name: kube-apiserver-heap\n      namespace: \"openshift-kube-apiserver\"\n      labelSelector: {app: openshift-kube-apiserver}\n      bearerToken: thisIsNotAValidToken\n      url: https://localhost:6443/debug/pprof/heap\n\n    - name: etcd-heap\n      namespace: \"openshift-etcd\"\n      labelSelector: {app: etcd}\n      certFile: etcd-peer-pert.crt\n      keyFile: etcd-peer-pert.key\n      url: https://localhost:2379/debug/pprof/heap\n</code></pre> <p>Warning</p> <p>As mentioned before, this measurement requires the <code>curl</code> command to be available in the target pods.</p>"},{"location":"measurements/#measure-subcommand-cli-example","title":"Measure subcommand CLI example","text":"<p>Measure subcommand example with relevant options. It is used to fetch measurements on top of resources that were a part of workload ran in past.</p> <pre><code>kube-burner measure --uuid=vchalla --namespaces=cluster-density-v2-0,cluster-density-v2-1,cluster-density-v2-2,cluster-density-v2-3,cluster-density-v2-4 --selector=kube-burner-job=cluster-density-v2\ntime=\"2023-11-19 17:46:05\" level=info msg=\"\ud83d\udcc1 Creating indexer: elastic\" file=\"kube-burner.go:226\"\ntime=\"2023-11-19 17:46:05\" level=info msg=\"map[kube-burner-job:cluster-density-v2]\" file=\"kube-burner.go:247\"\ntime=\"2023-11-19 17:46:05\" level=info msg=\"\ud83d\udcc8 Registered measurement: podLatency\" file=\"factory.go:85\"\ntime=\"2023-11-19 17:46:06\" level=info msg=\"Stopping measurement: podLatency\" file=\"factory.go:118\"\ntime=\"2023-11-19 17:46:06\" level=info msg=\"Evaluating latency thresholds\" file=\"metrics.go:60\"\ntime=\"2023-11-19 17:46:06\" level=info msg=\"Indexing pod latency data for job: kube-burner-measure\" file=\"pod_latency.go:245\"\ntime=\"2023-11-19 17:46:07\" level=info msg=\"Indexing finished in 417ms: created=4\" file=\"pod_latency.go:262\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"Indexing finished in 1.32s: created=50\" file=\"pod_latency.go:262\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"kube-burner-measure: PodScheduled 50th: 0 99th: 0 max: 0 avg: 0\" file=\"pod_latency.go:233\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"kube-burner-measure: ContainersReady 50th: 9000 99th: 18000 max: 18000 avg: 10680\" file=\"pod_latency.go:233\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"kube-burner-measure: Initialized 50th: 0 99th: 0 max: 0 avg: 0\" file=\"pod_latency.go:233\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"kube-burner-measure: Ready 50th: 9000 99th: 18000 max: 18000 avg: 10680\" file=\"pod_latency.go:233\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"Pod latencies error rate was: 0.00\" file=\"pod_latency.go:236\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"\ud83d\udc4b Exiting kube-burner vchalla\" file=\"kube-burner.go:209\"\n</code></pre>"},{"location":"measurements/#indexing-in-different-places","title":"Indexing in different places","text":"<p>The pod/vmi and service latency measurements send their metrics by default to all the indexers configured in the <code>metricsEndpoints</code> list, but it's possible to configure a different indexer for the quantile and the timeseries metrics by using the fields <code>quantilesIndexer</code> and <code>timeseriesIndexer</code>.</p> <p>For example</p> <pre><code>metricsEndpoints:\n- indexer:\n    type: local\n    alias: local-indexer\n- indexer:\n    type: opensearch\n    defaultIndex: kube-burner\n    esServers: [\"https://opensearch.domain:9200\"]\n    alias: os-indexer\nglobal:\n  measurements:\n  - name: podLatency\n    timeseriesIndexer: local-indexer\n    quantilesIndexer: os-indexer\n</code></pre> <p>With the configuration snippet above, the measurement <code>podLatency</code> would use the local indexer for timeseries metrics and opensearch for the quantile metrics.</p>"},{"location":"measurements/#additional-custom-measurements","title":"Additional Custom Measurements","text":"<p>kube-burner already implements core measurements. Additionally the <code>measurements</code> package exports interfaces, helper functions, and struct types to allow external consumers to implement custom measurements, interact with the measurement framework, and reuse common components.</p> <p>Additional measurement code has to:</p> <ol> <li>Implement the Measurement interface</li> <li>Create a new measurement factory with the previous and pass it as argument to RunWithAdditionalVars.</li> </ol> <pre><code>var additionalMeasurementFactoryMap = map[string]measurements.NewMeasurementFactory{\n        \"exampleLatency\": NewExampleLatencyMeasurementFactory,\n}\n\nwh = workloads.NewWorkloadHelper(workloadConfig, &amp;config, kubeClientProvider)\nrc = wh.RunWithAdditionalVars(workload, nil, additionalMeasurementFactoryMap)\n</code></pre>"},{"location":"observability/","title":"Overview","text":"<p>Performing a benchmark using kube-burner is relatively simple. However, it is sometimes necessary to analyze and be able to react to some KPIs in order to validate a benchmark. That is why kube-burner ships metric-collection and alerting systems based on Prometheus expressions.</p> <p>Kube-burner also ships an indexing feature that, in combination with the metric-collection and alerting features, can be used to analyze these KPIs in an external tool, such as Grafana or similar.</p> <p>The benchmark stages include the following:</p> <pre><code>flowchart TD\n    A[/\"Init benchmark\"/] -- Read config --&gt; B(Start measurements)\n    B --&gt; C(Run Job)\n    C --&gt; D{Next job?}\n    D --&gt; |Yes| C\n    D --&gt; |No| E[Stop measurements]\n    E --&gt; F[Evaluate alerts]\n    F --&gt; G[(Index results)]\n    G --&gt; H[Indexing]\n    H --&gt; I*[/End/]</code></pre>"},{"location":"observability/alerting/","title":"Alerting","text":"<p>Kube-burner includes an alerting feature able to evaluate Prometheus expressions in order to fire and index alerts.</p>"},{"location":"observability/alerting/#configuration","title":"Configuration","text":"<p>Alerting is configured through a configuration file pointed by the flag <code>--alert-profile</code> or <code>-a</code>, which is a YAML formatted file with the following shape:</p> <pre><code>- expr: avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[5m:]) &gt; 0.01\n  description: 5 minutes avg. etcd fsync latency on {{$labels.pod}} higher than 10ms {{$value}}\n  severity: error\n\n- expr: avg_over_time(histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))[5m:]) &gt; 0.1\n  description: 5 minutes avg. etcd netowrk peer round trip on {{$labels.pod}} higher than 100ms {{$value}}\n  severity: error\n\n- expr: increase(etcd_server_leader_changes_seen_total[2m]) &gt; 0\n  description: etcd leader changes observed\n  severity: error\n</code></pre> <p>Where <code>expr</code> holds the PromQL to evaluate and <code>description</code> holds a description of the alert, that will be printed/indexed when the alert fires. In the <code>description</code> field, you can use Prometheus labels to increase alert readability by using the syntax <code>{{$labels.&lt;label_name&gt;}}</code> and also print value of the value that fired the alarm using <code>{{$value}}</code>.</p> <p>You can configure alerts with a severity. Each severity level has different effects. These are:</p> <ul> <li><code>info</code>: Prints an info message with the alarm description to stdout. By default all expressions have this severity.</li> <li><code>warning</code>: Prints a warning message with the alarm description to stdout.</li> <li><code>error</code>: Prints an error message with the alarm description to stdout and makes kube-burner rc = 1</li> <li><code>critical</code>: Prints a fatal message with the alarm description to stdout and aborts execution immediately with rc =1 0</li> </ul>"},{"location":"observability/alerting/#using-the-elapsed-variable","title":"Using the elapsed variable","text":"<p>There is a special go-template variable that can be used within the Prometheus expression, the variable elapsed is set to the value of the job duration (or the range given to check-alerts). This variable is especially useful in expressions using aggregations over time functions. i.e:</p> <pre><code>- expr: avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[{{ .elapsed }}:]) &gt; 0.01\n  description: avg. etcd fsync latency on {{$labels.pod}} higher than 10ms {{$value}}\n  severity: error\n</code></pre>"},{"location":"observability/alerting/#checking-alerts","title":"Checking alerts","text":"<p>It is possible to look for alerts without triggering a kube-burner workload by using the <code>check-alerts</code> subcommand. Similar to the <code>index</code> CLI option, this option accepts the flags <code>--start</code> and <code>--end</code> to evaluate the alerts at a given time range.</p> <pre><code>$ kube-burner check-alerts -u https://prometheus.url.com -t ${token} -a alert-profile.yml\nINFO[2020-12-10 11:47:23] \ud83d\udc7d Initializing prometheus client\nINFO[2020-12-10 11:47:24] \ud83d\udd14 Initializing alert manager\nINFO[2020-12-10 11:47:24] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[5m:]) &gt; 0.01'\nERRO[2020-12-10 11:47:24] Alert triggered at 2020-12-10 11:01:53 +0100 CET: '5 minutes avg. etcd fsync latency on etcd-ip-10-0-213-209.us-west-2.compute.internal higher than 10ms 0.010281314285714311'\nINFO[2020-12-10 11:47:24] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))[5m:]) &gt; 0.1'\nINFO[2020-12-10 11:47:24] Evaluating expression: 'increase(etcd_server_leader_changes_seen_total[2m]) &gt; 0'\nINFO[2020-12-10 11:47:24] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, sum(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\",verb=~\"POST|PUT|DELETE|PATCH|CREATE\"}) by (verb,resource,subresource,le))[5m\n:]) &gt; 1'\nINFO[2020-12-10 11:47:25] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\",verb=\"GET\",scope=\"resource\"}[2m])) by (verb,resource,subresource,le))[5\nm:]) &gt; 1'\nINFO[2020-12-10 11:47:25] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\",verb=\"LIST\",scope=\"namespace\"}[2m])) by (verb,resource,subresource,le))\n[5m:]) &gt; 5'\nINFO[2020-12-10 11:47:26] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\",verb=\"LIST\",scope=\"cluster\"}[2m])) by (verb,resource,subresource,le))[5\nm:]) &gt; 30'\n</code></pre>"},{"location":"observability/alerting/#indexing-alerts","title":"Indexing alerts","text":"<p>When indexing is enabled, the alerts sent by kube-burner are automatically indexed by the provided <code>indexer</code>. The documents generated by these alerts have the following structure:</p> <pre><code>{\n  \"timestamp\": \"2023-01-19T22:20:10+01:00\",\n  \"uuid\": \"c0dd0d60-ddf5-488e-bf2f-b8960fc2b5ab\",\n  \"severity\": \"warning\",\n  \"description\": \"5 minutes avg. 99th etcd fsync latency on etcd-ip-10-0-133-30.us-west-2.compute.internal higher than 10ms. 0.004s\",\n  \"metricName\": \"alert\"\n}\n</code></pre>"},{"location":"observability/indexing/","title":"Indexing","text":"<p>Kube-burner can collect metrics and alerts from prometheus and from measurements and send them to the configured indexers.</p>"},{"location":"observability/indexing/#metrics-endpoints","title":"Metrics endpoints","text":"<p>The logic to configure metric collection and indexing is established by the <code>metricsEndpoints</code> field of the configuration file, this field is a list of objects with the following structure:</p> Field Description Example <code>endpoint</code> Define the prometheus endpoint to scrape <code>https://prom.my-domain.com</code> <code>username</code> Prometheus username (Basic auth) <code>username</code> <code>password</code> Prometheus password (Basic auth) <code>topSecret</code> <code>token</code> Prometheus bearer token (Bearer auth) <code>yourTokenDefinition</code> <code>step</code> Prometheus step size, used when scraping it, by default <code>30s</code> <code>1m</code> <code>skipTLSVerify</code> Skip TLS certificate verification, <code>true</code> by default <code>true</code> <code>metrics</code> List of metrics files <code>[metrics.yml, more-metrics.yml]</code> <code>alerts</code> List of alerts files <code>[alerts.yml, more-alerts.yml]</code> <code>indexer</code> Indexer configuration indexers <code>alias</code> Indexer alias, an arbitrary string required to send measurement results to an specific indexer <code>my-indexer</code> <p>Note</p> <p>Info about how to configure metrics-profiles and alerts-profiles</p>"},{"location":"observability/indexing/#indexers","title":"Indexers","text":"<p>Configured by the <code>indexer</code> field, it defines an indexer for the Prometheus endpoint, making all collected metrics to be indexed in it. Depending on the indexer, different configuration parameters need to be specified. The type of indexer is configured by the field <code>type</code></p> Option Description Supported values <code>type</code> Type of indexer <code>elastic</code>, <code>opensearch</code>, <code>local</code>"},{"location":"observability/indexing/#example","title":"Example","text":"<p>An example of how the <code>metricsEndpoints</code> section would look like in the configuration is:</p> <pre><code>metricsEndpoints:\n  - endpoint: http://localhost:9090\n    alias: os-indexing\n    alerts:\n    - alert-profile.yaml\n  - endpoint: https://remote-endpoint:9090\n    metrics:\n    - metrics-profile.yaml\n    indexer:\n      type: local\n      metricsDirectory: my-metrics\n</code></pre> <p>In the example above, two endpoints are defined, the first will be used to scrape for the alerts defined in the file <code>alert-profile.yaml</code> and the second one will be used to scrape the metrics defined in <code>metrics-profile.yaml</code>, these metrics will be indexed by the configured indexer.</p> <p>Info</p> <p>Configuring an indexer in an endpoint is only required when any metrics profile is configured</p>"},{"location":"observability/indexing/#elasticopensearch","title":"Elastic/OpenSearch","text":"<p>Send collected documents to Elasticsearch7 or OpenSearch instances.</p> <p>The <code>elastic</code> or <code>opensearch</code> indexer can be configured by the parameters below:</p> Option Description Type Default <code>esServers</code> List of Elasticsearch or OpenSearch URLs List [] <code>defaultIndex</code> Default index to send the Prometheus metrics into String \"\" <code>insecureSkipVerify</code> TLS certificate verification Boolean false <p>OpenSearch is backwards compatible with Elasticsearch and kube-burner does not use any version checks. Therefore, kube-burner with OpenSearch indexing should work as expected.</p> <p>Info</p> <p>It is possible to index documents in an authenticated Elasticsearch or OpenSearch instance using the notation <code>http(s)://[username]:[password]@[address]:[port]</code> in the <code>esServers</code> parameter.</p>"},{"location":"observability/indexing/#local","title":"Local","text":"<p>This indexer writes collected metrics to local files.</p> <p>The <code>local</code> indexer can be configured by the parameters below:</p> Option Description Type Default <code>metricsDirectory</code> Collected metric will be dumped here. String collected-metrics <code>createTarball</code> Create metrics tarball Boolean false <code>tarballName</code> Name of the metrics tarball String kube-burner-metrics.tgz"},{"location":"observability/indexing/#job-summary","title":"Job Summary","text":"<p>When an indexer is configured, a document holding the job summary is indexed at the end of the job. This is useful to identify the parameters the job was executed with. It also contains the timestamps of the execution phase (<code>timestamp</code> and <code>endTimestamp</code>) as well as the cleanup phase (<code>cleanupTimestamp</code> and <code>cleanupEndTimestamp</code>).</p> <p>This document looks like:</p> <pre><code>{\n  \"timestamp\": \"2023-08-29T00:17:27.942960538Z\",\n  \"endTimestamp\": \"2023-08-29T00:18:15.817272025Z\",\n  \"uuid\": \"83bfcb20-54f1-43f4-b2ad-ad04c2f4fd16\",\n  \"elapsedTime\": 48,\n  \"achievedQps\": 0.333,\n  \"cleanupTimestamp\": \"2023-08-29T00:18:18.015107794Z\",\n  \"cleanupEndTimestamp\": \"2023-08-29T00:18:49.014541929Z\",\n  \"metricName\": \"jobSummary\",\n  \"elapsedTime\": 8.768932955,\n  \"version\": \"v1.10.0\",\n  \"passed\": true,\n  \"executionErrors\": \"this is an example\",\n  \"jobConfig\": {                          \n    \"jobIterations\": 1,                                                                                              \n    \"name\": \"cluster-density-v2\",                                                                                    \n    \"jobType\": \"create\",                                                                                             \n    \"qps\": 20,                                                                                                       \n    \"burst\": 20,\n    \"namespace\": \"cluster-density-v2\",\n    \"maxWaitTimeout\": 14400000000000,\n    \"waitForDeletion\": true,\n    \"waitWhenFinished\": true,\n    \"cleanup\": true,\n    \"namespacedIterations\": true,\n    \"iterationsPerNamespace\": 1,\n    \"verifyObjects\": true,\n    \"errorOnVerify\": true,\n    \"preLoadImages\": true,\n    \"preLoadPeriod\": 15000000000,\n    \"churnPercent\": 10,\n    \"churnDuration\": 3600000000000,\n    \"churnDelay\": 120000000000,\n    \"churnDeletionStrategy\": \"default\"\n  }\n}\n</code></pre> <p>Note</p> <p>It's possible that some of the fields from the document above don't get indexed when it has no value</p>"},{"location":"observability/indexing/#metric-exporting-importing","title":"Metric exporting &amp; importing","text":"<p>When using the <code>local</code> indexer, it is possible to dump all of the collected metrics into a tarball, which you can import later. This is useful in disconnected environments, where kube-burner does not have direct access to an Elasticsearch instance. Metrics exporting can be configured by <code>createTarball</code> field of the indexer config as noted in the local indexer.</p> <p>The metric exporting feature is available through the <code>init</code> and <code>index</code> subcommands. Once you enabled it, a tarball (<code>kube-burner-metrics-&lt;timestamp&gt;.tgz</code>) containing all metrics is generated in the current working directory. This tarball can be imported and indexed by kube-burner with the <code>import</code> subcommand. For example:</p> <pre><code>$ kube-burner/bin/kube-burner import --config kubelet-config.yml --tarball kube-burner-metrics-1624441857.tgz\nINFO[2021-06-23 11:39:40] \ud83d\udcc1 Creating indexer: elastic\nINFO[2021-06-23 11:39:42] Importing tarball kube-burner-metrics-1624441857.tgz\nINFO[2021-06-23 11:39:42] Importing metrics from doc.json\nINFO[2021-06-23 11:39:43] Indexing [1] documents in kube-burner\nINFO[2021-06-23 11:39:43] Successfully indexed [1] documents in 208ms in kube-burner\n</code></pre>"},{"location":"observability/indexing/#scraping-from-multiple-endpoints","title":"Scraping from multiple endpoints","text":"<p>It is possible to scrape from multiple Prometheus endpoints and send the results to the target indexer with the <code>init</code> and <code>index</code> subcommands. This feature is configured by the flag <code>--metrics-endpoint</code>, which points to a YAML file with the required configuration.</p> <p>A valid file provided to the <code>--metrics-endpoint</code> looks like this:</p> <pre><code>- endpoint: http://localhost:9090 # This is one of the Prometheus endpoints\n  token: &lt;token&gt; # Authentication token\n  metrics: [metrics.yaml] # Metrics profiles to use for this endpoint\n  indexer:\n    - type: local\n- endpoint: http://remotehost:9090 # Another Prometheus endpoint\n  token: &lt;token&gt;\n  alerts: [alerts.yaml] # Alert profile, when metrics is not defined, defining an indexer is optional\n</code></pre> <p>Note</p> <p>The configuration provided by the <code>--metrics-endpoint</code> flag has precedence over the parameters specified in the config file.</p>"},{"location":"observability/metrics/","title":"Metric profile","text":"<p>The metric-collection feature is configured through a file pointed by the <code>metrics-profile</code> flag, which can point to a local path or URL of a YAML-formatted file containing a list of the Prometheus expressions. Kube-burner will perform those queries one by one, once all jobs are finished.</p> <p>In a single job benchmark, the queries are executed using the benchmark start and end time as time range. In multiple job benchmarks, these queries are executed in a per job basis, and they take the different start and end times from the executed jobs.</p> <p>The metrics profile file has the following structure:</p> <pre><code>- query: irate(process_cpu_seconds_total{job=~\".*(crio|etcd|controller-manager|apiserver|scheduler).*\"}[2m])\n  metricName: controlPlaneCPU\n\n- query: sum(irate(node_cpu_seconds_total[2m])) by (mode,instance)\n  metricName: nodeCPU\n</code></pre> <p>The <code>query</code> field holds the Prometheus expression to evaluate, and <code>metricName</code> controls the value that kube-burner will set on the <code>metricName</code> field of the generated documents. This is useful to identify metrics from a specific query. More information is available in the metric format section.</p>"},{"location":"observability/metrics/#instant-queries","title":"Instant queries","text":"<p>In addition to the default range queries, kube-burner has the ability execute instant queries against the provided Prometheus API. This can be configured by enabling the field <code>instant</code> to the desired metric.</p> <pre><code>- query: kube_node_role\n  metricName: nodeRoles\n  instant: true\n</code></pre> <p>Info</p> <p>When using instant queries, the generated documents are resulting from scraping the last timestamp of each job. It is possible to generate an extra document resulting from scraping the first timestamp of the jobs by adding <code>captureStart: true</code> to the metric definition, the resulting document's <code>metricName</code> are appended the <code>-start</code> suffix.</p>"},{"location":"observability/metrics/#metric-format","title":"Metric format","text":"<p>The collected metrics have the following shape:</p> <pre><code>[\n  {\n    \"timestamp\": \"2021-06-23T11:50:15+02:00\",\n    \"labels\": {\n      \"instance\": \"ip-10-0-219-170.eu-west-3.compute.internal\",\n      \"mode\": \"user\"\n    },\n    \"value\": 0.3300880234732172,\n    \"uuid\": \"&lt;UUID&gt;\",\n    \"query\": \"sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) &gt; 0\",\n    \"metricName\": \"nodeCPU\",\n  },\n  {\n    \"timestamp\": \"2021-06-23T11:50:45+02:00\",\n    \"labels\": {\n      \"instance\": \"ip-10-0-219-170.eu-west-3.compute.internal\",\n      \"mode\": \"user\"\n    },\n    \"value\": 0.31978102677038506,\n    \"uuid\": \"&lt;UUID&gt;\",\n    \"query\": \"sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) &gt; 0\",\n    \"metricName\": \"nodeCPU\",\n  }\n]\n</code></pre> <p>Notice that kube-burner enriches the query results by adding some extra fields like <code>uuid</code>, <code>query</code> and <code>metricName</code>.</p> <p>Info</p> <p>These extra fields are especially useful at the time of identifying and representing the collected metrics.</p>"},{"location":"observability/metrics/#using-the-elapsed-variable","title":"Using the elapsed variable","text":"<p>There is a special go-template variable that can be used within the Prometheus expressions of a metric profile; the variable <code>elapsed</code> is automatically populated with the job duration, in seconds. This variable is especially useful in PromQL expressions using aggregations over time functions.</p> <p>For example, the following expression gets the top 3 datapoints with the average CPU usage kubelets processes in the cluster.</p> <pre><code>- query: irate(process_cpu_seconds_total{service=\"kubelet\",job=\"kubelet\"}[2m]) * 100 and on (node) topk(3,avg_over_time(irate(process_cpu_seconds_total{service=\"kubelet\",job=\"kubelet\"}[2m])[{{ .elapsed }}:]))\n  metricName: top3KubeletCPU\n  instant: true\n</code></pre> <p>Info</p> <p>Note that in the [time-range:] notation, the colon specifies to get the values for the given duration.</p> <p>Examples of metrics profiles can be found in the examples directory. There are also Elasticsearch based Grafana dashboards available in the same examples directory.</p>"},{"location":"reference/configuration/","title":"Reference","text":"<p>All of the magic that <code>kube-burner</code> does is described in its configuration file. As previously mentioned, the location of this configuration file is provided by the flag <code>-c</code>. This flag points to a YAML-formatted file that consists of several sections.</p>"},{"location":"reference/configuration/#templating-the-configuraion-file","title":"Templating the configuraion file","text":"<p>go-template semantics may be used within the configuration file. The input for the templates is taken from a user data file (using the <code>--user-data</code> parameter) and/or environment variables. Environment variables take precedence over those defined in the file when the same variable is defined in both.</p> <p>For example, you could define the <code>indexers</code> section of your own configuration file, such as:</p> <pre><code>metricsEndpoints:\n{{ if .OS_INDEXING }}\n  - prometheusURL: http://localhost:9090\n    indexer:\n      type: opensearch\n      esServers: [\"{{ .ES_SERVER }}\"]\n      defaultIndex: {{ .ES_INDEX }}\n{{ end }}\n{{ if .LOCAL_INDEXING }}\n  - prometheusURL: http://localhost:9090\n    indexer:\n      type: local\n      metricsDirectory: {{ .METRICS_FOLDER }}\n{{ end }}\n</code></pre> <p>This feature can be very useful at the time of defining secrets, such as the user and password of our indexer, or a token to use in pprof collection.</p>"},{"location":"reference/configuration/#global","title":"Global","text":"<p>In this section is described global job configuration, it holds the following parameters:</p> Option Description Type Default <code>measurements</code> List of measurements. Detailed in the measurements section List [] <code>requestTimeout</code> Client-go request timeout Duration 60s <code>gc</code> Garbage collect created namespaces Boolean false <code>gcMetrics</code> Flag to collect metrics during garbage collection Boolean false <code>gcTimeout</code> Garbage collection timeout Duration 1h <code>waitWhenFinished</code> Wait for all pods/jobs (including probes) to be running/completed when all jobs are completed Boolean false <code>clusterHealth</code> Checks if all the nodes are in \"Ready\" state Boolean false <code>timeout</code> Global benchmark timeout Duration 4hr <code>functionTemplates</code> Function template files to render at runtime List [] <p>Note</p> <p>The precedence order to wait on resources is Global.waitWhenFinished &gt; Job.waitWhenFinished &gt; Job.podWait</p> <p>Warning</p> <p>Global <code>waitWhenFinished</code> and job <code>gc</code> are mutually exclusive and cannot be enabled at the same time.</p> <p>kube-burner connects k8s clusters using the following methods in this order:</p> <ul> <li><code>KUBECONFIG</code> environment variable</li> <li><code>$HOME/.kube/config</code></li> <li>In-cluster config (Used when kube-burner runs inside a pod)</li> </ul>"},{"location":"reference/configuration/#function-templating-example","title":"Function templating example","text":"<p>Using function templates we can define a block of code as function and reuse it in any parts of our configuration. For the purpose of this example, lets assume we have a configuration like below in our deployment.yaml <pre><code>env:\n- name: ENVVAR1_{{.name}}\n  value: {{.envVar}}\n- name: ENVVAR2_{{.name}}\n  value: {{.envVar}}\n- name: ENVVAR3_{{.name}}\n  value: {{.envVar}}\n- name: ENVVAR4_{{.name}}\n  value: {{.envVar}}\n</code></pre> Now I want to modularize and use it in my code. In order to do that, I will create a template envs.tpl as below. <pre><code>{{- define \"env_func\" -}}\n{{- range $i := until $.n }}\n{{- printf \"- name: ENVVAR%d_%s\\n  value: %s\" (add $i 1) $.name $.envVar | nindent $.indent }}\n{{- end }}\n{{- end }}\n</code></pre> Once done we will make sure that our function template is invoked as a part of the global configuration as below so that it can be used across. <pre><code>global:\n  functionTemplates:\n    - envs.tpl\n</code></pre> Final step is to invoke this function with required parameters to make sure it replaces the redundant code in our deployment.yaml file. <pre><code>env:\n{{- template \"env_func\" (dict \"name\" .name \"envVar\" .envVar \"n\" 4 \"indent\" 8) }}\n</code></pre> We are all set! We should have our function rendered at the runtime and can be reused in future as well.</p>"},{"location":"reference/configuration/#jobs","title":"Jobs","text":"<p>This section contains the list of jobs <code>kube-burner</code> will execute. Each job can hold the following parameters.</p> Option Description Type Default <code>name</code> Job name String \"\" <code>jobType</code> Type of job to execute. More details at job types String create <code>jobIterations</code> How many times to execute the job Integer 0 <code>namespace</code> Namespace base name to use String \"\" <code>namespacedIterations</code> Whether to create a namespace per job iteration Boolean true <code>iterationsPerNamespace</code> The maximum number of <code>jobIterations</code> to create in a single namespace. Important for node-density workloads that create Services. Integer 1 <code>cleanup</code> Cleanup clean up old namespaces Boolean true <code>podWait</code> Wait for all pods/jobs (including probes) to be running/completed before moving forward to the next job iteration Boolean false <code>waitWhenFinished</code> Wait for all pods/jobs (including probes) to be running/completed when all job iterations are completed Boolean true <code>maxWaitTimeout</code> Maximum wait timeout per namespace Duration 4h <code>jobIterationDelay</code> How long to wait between each job iteration. This is also the wait interval between each delete operation Duration 0s <code>jobPause</code> How long to pause after finishing the job Duration 0s <code>beforeCleanup</code> Allows to run a bash script before the workload is deleted String \"\" <code>gc</code> Garbage collect job Boolean false <code>qps</code> Limit object creation queries per second Integer 0 <code>burst</code> Maximum burst for throttle Integer 0 <code>objects</code> List of objects the job will create. Detailed on the objects section List [] <code>watchers</code> List of watchers to be created for the job. Detailed on the watchers section List [] <code>verifyObjects</code> Verify object count after running each job Boolean true <code>errorOnVerify</code> Set RC to 1 when objects verification fails Boolean true <code>skipIndexing</code> Skip metric indexing on this job Boolean false <code>preLoadImages</code> Kube-burner will create a DS before triggering the job to pull all the images of the job Boolean <code>preLoadPeriod</code> How long to wait for the preload DaemonSet Duration 1m <code>preloadNodeLabels</code> Add node selector labels for the resources created in preload stage Object {} <code>namespaceLabels</code> Add custom labels to the namespaces created by kube-burner Object {} <code>namespaceAnnotations</code> Add custom annotations to the namespaces created by kube-burner Object {} <code>churn</code> Churn the workload. Only supports namespace based workloads Boolean false <code>churnCycles</code> Number of churn cycles to execute Integer 100 <code>churnPercent</code> Percentage of the jobIterations to churn each period Integer 10 <code>churnDuration</code> Length of time that the job is churned for Duration 1h <code>churnDelay</code> Length of time to wait between each churn period Duration 5m <code>churnDeletionStrategy</code> Churn deletion strategy to apply, <code>default</code> or <code>gvr</code> (where <code>default</code> churns namespaces and <code>gvr</code> churns objects within namespaces) String default <code>defaultMissingKeysWithZero</code> Stops templates from exiting with an error when a missing key is found, meaning users will have to ensure templates hand missing keys Boolean false <code>executionMode</code> Job execution mode. More details at execution modes String parallel <code>objectDelay</code> How long to wait between each object in a job Duration 0s <code>objectWait</code> Wait for each object to complete before processing the next one - not for Create jobs Boolean 0s <code>metricsAggregate</code> Aggregate the metrics collected for this job with those of the next one Boolean false <code>metricsClosing</code> To define when the metrics collection should stop. More details at MetricsClosing String afterJobPause <p>Note</p> <p>Both <code>churnCycles</code> and <code>churnDuration</code> serve as termination conditions, with the churn process halting when either condition is met first. If someone wishes to exclusively utilize <code>churnDuration</code> to control churn, they can achieve this by setting <code>churnCycles</code> to <code>0</code>. Conversely, to prioritize <code>churnCycles</code>, one should set a longer <code>churnDuration</code> accordingly.</p> <p>Note</p> <p>When <code>jobType</code> is set to Delete the following settings are forced: <code>jobIterations</code> is set to <code>1</code>, <code>waitWhenFinished</code> is set to <code>false</code>, <code>executionMode</code> is set to <code>sequential</code></p> <p>Our configuration files strictly follow YAML syntax. To clarify on List and Object types usage, they are nothing but the <code>Lists and Dictionaries</code> in YAML syntax.</p> <p>Examples of valid configuration files can be found in the examples folder.</p>"},{"location":"reference/configuration/#watchers","title":"Watchers","text":"<p>We have watchers support during the benchmark workload. It is at a job level and will be usefull in scenarios where we want to monitor overhead created by watchers on a cluster.</p> <p>Note</p> <p>This feature doesn't effect the overall QPS/Burst as it uses its own client instance.</p> Option Description Type Default <code>kind</code> Object kind to consider for watch String \"\" <code>apiVersion</code> Object apiVersion to consider for watch String \"\" <code>labelSelector</code> Objects with these labels will be considered for watch Object {} <code>replicas</code> Number of watcher replicas to create Integer 0"},{"location":"reference/configuration/#objects","title":"Objects","text":"<p>The objects created by <code>kube-burner</code> are rendered using the default golang's template library. Each object element supports the following parameters:</p> Option Description Type Default <code>objectTemplate</code> Object template file path or URL String \"\" <code>replicas</code> How replicas of this object to create per job iteration Integer - <code>inputVars</code> Map of arbitrary input variables to inject to the object template Object - <code>wait</code> Wait for object to be ready Boolean true <code>waitOptions</code> Customize how to wait for object to be ready Object {} <code>runOnce</code> Create or delete this object only once during the entire job Boolean false <p>Warning</p> <p>Kube-burner is only able to wait for a subset of resources, unless <code>waitOptions</code> are specified.</p>"},{"location":"reference/configuration/#built-in-support-for-object-waiters","title":"Built-in support for object waiters","text":"<p>The following object types have built-in waiters: - StatefulSet - Deployment - DaemonSet - ReplicaSet - Job - Pod - ReplicationController - Build - BuildConfig - VirtualMachine - VirtualMachineInstance - VirtualMachineInstanceReplicaSet - PersistentVolumeClaim - VolumeSnapshot - DataVolume - DataSource</p> <p>Info</p> <p>Find more info about the waiters implementation in the <code>pkg/burner/waiters.go</code> file</p>"},{"location":"reference/configuration/#object-wait-options","title":"Object wait Options","text":"<p>If you want to override the default waiter behaviors, you can specify wait options for your objects.</p> Option Description Type Default <code>apiVersion</code> Object apiVersion to consider for wait String \"\" <code>kind</code> Object kind to consider for wait String \"\" <code>labelSelector</code> Objects with these labels will be considered for wait Object {} <code>customStatusPaths</code> list of jq path/values to verify readiness of the object Object [] <p>For example, the snippet below can be used to make kube-burner wait for all containers from the pod defined at <code>pod.yml</code> to be ready.</p> <pre><code>objects:\n- objectTemplate: deployment.yml\n  replicas: 3\n  waitOptions:\n    kind: Pod\n    labelSelector: {kube-burner-label : abcd}\n</code></pre> <p>Additionally, you can use <code>customStatusPaths</code> to specify custom paths to be checked for the readiness of the object. For example, to wait for a deployment to be available</p> <p><pre><code>objects:\n  - kind: Deployment\n    objectTemplate: deployment.yml\n    replicas: 1\n    waitOptions:\n      customStatusPaths:\n        - key: '(.conditions.[] | select(.type == \"Available\")).status'\n          value: \"True\"\n</code></pre> This allows kube-burner to check the status at all the specified key/value pairs and verify readiness of the object. If any of them do not match then it is indicated as a failure.</p> <p>Note</p> <p>Currently, the <code>value</code> field expects only strings.   In order to test other types make sure to convert the result to a string in the <code>key</code>.</p> <p>For example, to verify that a <code>VolumeSnapshot</code> is <code>readyToUse</code> set the <code>customStatusPaths</code> to:   <pre><code>customStatusPaths:\n  - key: '(.conditions.[] | select(.type == \"Ready\")).status'\n    value: \"True\"\n</code></pre></p> <p>Note</p> <p><code>waitOptions.kind</code>, <code>waitOptions.customStatusPaths</code> and <code>waitOptions.labelSelector</code> are fully optional. <code>waitOptions.kind</code> is used when an application has child objects to be waited &amp; <code>waitOptions.labelSelector</code> is used when we want to wait on objects with specific labels.</p>"},{"location":"reference/configuration/#default-labels","title":"Default labels","text":"<p>All objects created by kube-burner are labeled with <code>kube-burner-uuid=&lt;UUID&gt;,kube-burner-job=&lt;jobName&gt;,kube-burner-index=&lt;objectIndex&gt;</code>. They are used for internal purposes, but they can also be used by the users.</p>"},{"location":"reference/configuration/#job-types","title":"Job types","text":"<p>Configured by the parameter <code>jobType</code>, kube-burner supports four types of jobs with different parameters each:</p> <ul> <li>Create</li> <li>Delete</li> <li>Read</li> <li>Patch</li> </ul>"},{"location":"reference/configuration/#create","title":"Create","text":"<p>The default <code>jobType</code> is create. Creates objects listed in the <code>objects</code> list as described in the objects section. The amount of objects created is configured by <code>jobIterations</code>, <code>replicas</code>. If the object is namespaced and has an empty <code>.metadata.namespace</code> field, <code>kube-burner</code> creates a new namespace with the name <code>namespace-&lt;iteration&gt;</code>, and creates the defined amount of objects in it.</p>"},{"location":"reference/configuration/#delete","title":"Delete","text":"<p>This type of job deletes objects described in the objects list. Using delete as job type the objects list would have the following structure:</p> <pre><code>objects:\n- kind: Deployment\n  labelSelector: {kube-burner-job: cluster-density}\n  apiVersion: apps/v1\n\n- kind: Secret\n  labelSelector: {kube-burner-job: cluster-density}\n</code></pre> <p>Where:</p> <ul> <li><code>kind</code>: Object kind of the k8s object to delete.</li> <li><code>labelSelector</code>: Deletes the objects with the given labels.</li> <li><code>apiVersion</code>: API version from the k8s object.</li> </ul> <p>This type of job supports the following parameters. Described in the jobs section:</p> <ul> <li><code>waitForDeletion</code>: Wait for objects to be deleted before finishing the job. Defaults to <code>true</code>.</li> <li><code>name</code></li> <li><code>qps</code></li> <li><code>burst</code></li> <li><code>jobPause</code></li> <li><code>jobIterationDelay</code></li> </ul>"},{"location":"reference/configuration/#read","title":"Read","text":"<p>This type of job reads objects described in the objects list. Using read as job type the objects list would have the following structure:</p> <pre><code>objects:\n- kind: Deployment\n  labelSelector: {kube-burner-job: cluster-density}\n  apiVersion: apps/v1\n\n- kind: Secret\n  labelSelector: {kube-burner-job: cluster-density}\n</code></pre> <p>Where:</p> <ul> <li><code>kind</code>: Object kind of the k8s object to read.</li> <li><code>labelSelector</code>: Reads the objects with the given labels.</li> <li><code>apiVersion</code>: API version from the k8s object.</li> </ul> <p>This type of job supports the following parameters. Described in the jobs section:</p> <ul> <li><code>name</code></li> <li><code>qps</code></li> <li><code>burst</code></li> <li><code>jobPause</code></li> <li><code>jobIterationDelay</code></li> <li><code>jobIterations</code></li> </ul>"},{"location":"reference/configuration/#patch","title":"Patch","text":"<p>This type of job can be used to patch objects with the template described in the object list. This object list has the following structure:</p> <pre><code>objects:\n- kind: Deployment\n  labelSelector: {kube-burner-job: cluster-density}\n  objectTemplate: templates/deployment_patch_add_label.json\n  patchType: \"application/strategic-merge-patch+json\"\n  apiVersion: apps/v1\n</code></pre> <p>Where:</p> <ul> <li><code>kind</code>: Object kind of the k8s object to patch.</li> <li><code>labelSelector</code>: Map with the labelSelector.</li> <li><code>objectTemplate</code>: The YAML template or JSON file to patch.</li> <li><code>apiVersion</code>: API version from the k8s object.</li> <li><code>patchType</code>: The Kubernetes request patch type (see below).</li> </ul> <p>Valid patch types:</p> <ul> <li>application/json-patch+json</li> <li>application/merge-patch+json</li> <li>application/strategic-merge-patch+json</li> <li>application/apply-patch+yaml (requires YAML)</li> </ul> <p>As mentioned previously, all objects created by kube-burner are labeled with <code>kube-burner-uuid=&lt;UUID&gt;,kube-burner-job=&lt;jobName&gt;,kube-burner-index=&lt;objectIndex&gt;</code>. Therefore, you can design a workload with one job to create objects and another one to patch or remove the objects created by the previous.</p> <pre><code>jobs:\n- name: create-objects\n  namespace: job-namespace\n  jobIterations: 100\n  objects:\n  - objectTemplate: deployment.yml\n    replicas: 10\n\n  - objectTemplate: service.yml\n    replicas: 10\n\n- name: remove-objects\n  jobType: delete\n  objects:\n  - kind: Deployment\n    labelSelector: {kube-burner-job: create-objects}\n    apiVersion: apps/v1\n\n  - kind: Secret\n    labelSelector: {kube-burner-job: create-objects}\n</code></pre>"},{"location":"reference/configuration/#kubevirt","title":"Kubevirt","text":"<p>This type of job can be used to execute <code>virtctl</code> commands described in the object list. This object list has the following structure:</p> <pre><code>objects:\n- kubeVirtOp: start\n  labelSelector: {kube-burner-job: cluster-density}\n  inputVars:\n    force: true\n</code></pre> <p>Where:</p> <ul> <li><code>kubeVirtOp</code>: virtctl operation to execute.</li> <li><code>labelSelector</code>: Map with the labelSelector.</li> <li><code>inputVars</code>: Additional command parameters</li> </ul>"},{"location":"reference/configuration/#supported-operations","title":"Supported Operations","text":""},{"location":"reference/configuration/#start","title":"<code>start</code>","text":"<p>Execute <code>virtctl start</code> on the VMs mapped by the <code>labelSelector</code>. Additional parameters may be set using the <code>inputVars</code> field:</p> <ul> <li><code>startPaused</code> - VM will start in <code>Paused</code> state. Default <code>false</code></li> </ul>"},{"location":"reference/configuration/#stop","title":"<code>stop</code>","text":"<p>Execute <code>virtctl stop</code> on the VMs mapped by the <code>labelSelector</code>. Additional parameters may be set using the <code>inputVars</code> field:</p> <ul> <li><code>force</code> - Force stop the VM without waiting. Default <code>false</code></li> </ul>"},{"location":"reference/configuration/#restart","title":"<code>restart</code>","text":"<p>Execute <code>virtctl restart</code> on the VMs mapped by the <code>labelSelector</code>. Additional parameters may be set using the <code>inputVars</code> field:</p> <ul> <li><code>force</code> - Force restart the VM without waiting. Default <code>false</code></li> </ul>"},{"location":"reference/configuration/#pause","title":"<code>pause</code>","text":"<p>Execute <code>virtctl pause</code> on the VMs mapped by the <code>labelSelector</code>. No additional parametes are supported.</p>"},{"location":"reference/configuration/#unpause","title":"<code>unpause</code>","text":"<p>Execute <code>virtctl unpause</code> on the VMs mapped by the <code>labelSelector</code>. No additional parametes are supported.</p>"},{"location":"reference/configuration/#migrate","title":"<code>migrate</code>","text":"<p>Execute <code>virtctl migrate</code> on the VMs mapped by the <code>labelSelector</code>. No additional parametes are supported.</p>"},{"location":"reference/configuration/#add-volume","title":"<code>add-volume</code>","text":"<p>Execute <code>virtctl addvolume</code> on the VMs mapped by the <code>labelSelector</code>. Additional parameters should be set using the <code>inputVars</code> field:</p> <ul> <li><code>volumeName</code> - Name of the already existing volume to add. Mandatory</li> <li><code>diskType</code> - Type of the new volume (<code>disk</code>/<code>lun</code>). Default <code>disk</code></li> <li><code>serial</code> - serial number you want to assign to the disk. Defaults to the value of <code>volumeName</code></li> <li><code>cache</code> - caching options attribute control the cache mechanism. Default <code>''</code></li> <li><code>persist</code> - if set, the added volume will be persisted in the VM spec (if it exists). Default <code>false</code></li> </ul>"},{"location":"reference/configuration/#remove-volume","title":"<code>remove-volume</code>","text":"<p>Execute <code>virtctl removevolume</code> on the VMs mapped by the <code>labelSelector</code>. Additional parameters should be set using the <code>inputVars</code> field:</p> <ul> <li><code>volumeName</code> - Name of the volume to remove. Mandatory</li> <li><code>persist</code> - if set, the added volume will be persisted in the VM spec (if it exists). Default <code>false</code></li> </ul>"},{"location":"reference/configuration/#wait-for-completion","title":"Wait for completion","text":"<p>Wait is supported for the following operations:</p> <ul> <li><code>start</code> - Wait for the <code>Ready</code> state of the <code>VirtualMachine</code>  to become <code>True</code></li> <li><code>stop</code> - Wait for the  <code>Ready</code> state of the <code>VirtualMachine</code> state to become <code>False</code> with <code>reason</code> equal to <code>VMINotExists</code></li> <li><code>restart</code> - Wait for the <code>Ready</code> state of the <code>VirtualMachine</code>  to become <code>True</code></li> <li><code>pause</code> - Wait for the <code>Paused</code> state of the <code>VirtualMachine</code>  to become <code>True</code></li> <li><code>unpause</code> - Wait for the <code>Ready</code> state of the <code>VirtualMachine</code>  to become <code>True</code></li> <li><code>migrate</code> - Wait for the <code>Ready</code> state of the <code>VirtualMachine</code>  to become <code>True</code></li> </ul> <p>Note</p> <p>The waiter makes sure that the <code>lastTransitionTime</code> of the condition is after the time of the command. This requires that the timestamps on the cluster side are in UTC</p>"},{"location":"reference/configuration/#execution-modes","title":"Execution Modes","text":"<p>Patch jobs support different execution modes</p> <ul> <li><code>parallel</code> - run all steps without any waiting between objects or iterations</li> <li><code>sequential</code> - Run for each object before moving to the next job iteration with an optional wait between objects and/or between iterations</li> </ul>"},{"location":"reference/configuration/#churning-jobs","title":"Churning Jobs","text":"<p>Churn is the deletion and re-creation of objects, and is supported for namespace-based jobs only. This occurs after the job has completed but prior to uploading metrics, if applicable. It deletes a percentage of contiguous namespaces randomly chosen and re-creates them with all of the appropriate objects. It will then wait for a specified delay (or none if set to <code>0</code>) before deleting and recreating the next randomly chosen set. This cycle continues until the churn duration has passed.</p> <p>An example implementation that would churn 20% of the 100 job iterations for 2 hours with no delay between sets:</p> <pre><code>jobs:\n- name: cluster-density\n  jobIterations: 100\n  namespacedIterations: true\n  namespace: churning\n  churn: true\n  churnPercent: 20\n  churnDuration: 2h\n  churnDelay: 0s\n  objects:\n  - objectTemplate: deployment.yml\n    replicas: 10\n\n  - objectTemplate: service.yml\n    replicas: 10\n</code></pre>"},{"location":"reference/configuration/#injected-variables","title":"Injected variables","text":"<p>All object templates are injected with the variables below by default:</p> <ul> <li><code>Iteration</code>: Job iteration number.</li> <li><code>Replica</code>: Object replica number. Keep in mind that this number is reset to 1 with each job iteration.</li> <li><code>JobName</code>: Job name.</li> <li><code>UUID</code>: Benchmark UUID.</li> <li><code>RunID</code>: Internal run id. Can be used to match resources for metrics collection</li> </ul> <p>In addition, you can also inject arbitrary variables with the option <code>inputVars</code> of the object:</p> <pre><code>- objectTemplate: service.yml\n  replicas: 2\n  inputVars:\n    port: 80\n    targetPort: 8080\n</code></pre> <p>The following code snippet shows an example of a k8s service using these variables:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: sleep-app-{{.Iteration}}-{{.Replica}}\n  labels:\n    name: my-app-{{.Iteration}}-{{.Replica}}\nspec:\n  selector:\n    app: sleep-app-{{.Iteration}}-{{.Replica}}\n  ports:\n  - name: serviceport\n    protocol: TCP\n    port: \"{{.port}}\"\n    targetPort: \"{{.targetPort}}\"\n  type: ClusterIP\n</code></pre> <p>You can also use golang template semantics in your <code>objectTemplate</code> definitions</p> <pre><code>kind: ImageStream\napiVersion: image.openshift.io/v1\nmetadata:\n  name: {{.prefix}}-{{.Replica}}\nspec:\n{{ if .image }}\n  dockerImageRepository: {{.image}}\n{{ end }}\n</code></pre>"},{"location":"reference/configuration/#template-functions","title":"Template functions","text":"<p>On top of the default golang template semantics, <code>kube-burner</code> supports additional template functions.</p>"},{"location":"reference/configuration/#external-libraries","title":"External libraries","text":"<ul> <li>sprig library which adds over 70 template functions for Go\u2019s template language.</li> </ul>"},{"location":"reference/configuration/#additional-functions","title":"Additional functions","text":"<ul> <li><code>Binomial</code> - returns the binomial coefficient of (n,k)</li> <li><code>IndexToCombination</code> - returns the combination corresponding to the given index</li> <li><code>GetSubnet24</code></li> <li><code>GetIPAddress</code> - returns number of addresses requested per iteration from the list of total provided addresses</li> <li><code>ReadFile</code> - returns the content of the file in the provided path</li> </ul>"},{"location":"reference/configuration/#runonce","title":"RunOnce","text":"<p>All objects within the job will iteratively run based on the JobIteration number, but there may be a situation if an object need to be created only once (ex. clusterrole), in such cases we can add an optional field as <code>runOnce</code> for that particular object to execute only once in the entire job.</p> <p>An example scenario as below template, a job iteration of 100 but create the clusterrole only once.</p> <pre><code>jobs:\n- name: cluster-density\n  jobIterations: 100\n  namespacedIterations: true\n  namespace: cluster-density\n  objects:\n  - objectTemplate: clusterrole.yml\n    replicas: 1\n    runOnce: true\n\n  - objectTemplate: clusterrolebinding.yml\n    replicas: 1\n    runOnce: true\n\n  - objectTemplate: deployment.yml\n    replicas: 10\n</code></pre>"},{"location":"reference/configuration/#metricsclosing","title":"MetricsClosing","text":"<p>This config defines when the metrics collection should stop. The option supports three values:</p> <ul> <li><code>afterJob</code> - collect metrics after the job completes</li> <li><code>afterJobPause</code> - collect metrics after the jobPause duration ends (Default)</li> <li><code>afterMeasurements</code> - collect metrics after all measurements are finished</li> </ul>"},{"location":"wrappers/wrappers/","title":"Kube-burner wrappers","text":"<p>It's possible to extend and take advantage of the kube-burner's features by developing wrappers. A kube-burner wrapper is basically a new binary that uses some of the exposed functions and interfaces of kube-burner.</p>"},{"location":"wrappers/wrappers/#helper-functions","title":"Helper functions","text":"<p>There're some helper functions meant to be consumed by wrappers in the <code>workloads</code> package. These functions provide some shortcuts to incorporate workloads in golang embedded filesystems.</p> <p>In order to start a new wrapper you need to instantiate a new <code>WorkloadHelper</code> by calling <code>workloads.NewWorkloadHelper()</code>.</p> <pre><code>wh := workloads.NewWorkloadHelper(config.Config, embedFS, \"workloads\", \"metrics\", \"alerts\", \"scripts\", &amp;config.KubeClientProvider)\nreturnCode := wh.Run(\"workload.yaml\")\n</code></pre>"},{"location":"wrappers/wrappers/#customizing-template-rendering","title":"Customizing template rendering","text":"<p>It's possible to provide custom template rendering functions from a wrapper, this can be done by calling the function <code>AddRenderingFunction()</code> of the util package. For example:</p> <pre><code>util.AddRenderingFunction(\"isEven\", func(n int) bool {\n    return n % 2 == 0\n})\n</code></pre> <p>With the above code snippet we're provisioning a new template rendering function <code>isEven</code>, that basically returns true or false depending if the provided number is even. This function can be consumed in any of the kube-burner configuration files, for example</p> <pre><code>jobs:\n  - name: job\n    podWait: {{ isEven 5 }}\n</code></pre>"}]}